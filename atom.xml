<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>时光琥珀</title>
  
  <subtitle>true</subtitle>
  <link href="http://example.com/atom.xml" rel="self"/>
  
  <link href="http://example.com/"/>
  <updated>2024-04-25T07:37:38.436Z</updated>
  <id>http://example.com/</id>
  
  <author>
    <name>庄生晓梦</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>mobildnerf</title>
    <link href="http://example.com/2024/04/25/mobildnerf/"/>
    <id>http://example.com/2024/04/25/mobildnerf/</id>
    <published>2024-04-25T07:17:16.000Z</published>
    <updated>2024-04-25T07:37:38.436Z</updated>
    
    <content type="html"><![CDATA[<h1 id="MobileNeRF-Exploiting-the-Polygon-Rasterization-Pipeline-for-Efficient-Neural-Field-Rendering-on-Mobile-Architectures"><a href="#MobileNeRF-Exploiting-the-Polygon-Rasterization-Pipeline-for-Efficient-Neural-Field-Rendering-on-Mobile-Architectures" class="headerlink" title="MobileNeRF: Exploiting the Polygon Rasterization Pipeline for Efficient Neural Field Rendering on Mobile Architectures"></a>MobileNeRF: Exploiting the Polygon Rasterization Pipeline for Efficient Neural Field Rendering on Mobile Architectures</h1><p><a href="https://mobile-nerf.github.io/">MobileNeRF (mobile-nerf.github.io)</a></p><h1 id="施工ing"><a href="#施工ing" class="headerlink" title="施工ing"></a>施工ing</h1>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;MobileNeRF-Exploiting-the-Polygon-Rasterization-Pipeline-for-Efficient-Neural-Field-Rendering-on-Mobile-Architectures&quot;&gt;&lt;a href=&quot;#Mob</summary>
      
    
    
    
    <category term="CV" scheme="http://example.com/categories/CV/"/>
    
    
    <category term="CV" scheme="http://example.com/tags/CV/"/>
    
    <category term="NeRF" scheme="http://example.com/tags/NeRF/"/>
    
    <category term="神经渲染" scheme="http://example.com/tags/%E7%A5%9E%E7%BB%8F%E6%B8%B2%E6%9F%93/"/>
    
  </entry>
  
  <entry>
    <title>Instant-NGP</title>
    <link href="http://example.com/2024/04/25/NGP/"/>
    <id>http://example.com/2024/04/25/NGP/</id>
    <published>2024-04-25T07:16:59.000Z</published>
    <updated>2024-04-25T07:33:01.307Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Instant-Neural-Graphics-Primitives-with-a-Multiresolution-Hash-Encoding"><a href="#Instant-Neural-Graphics-Primitives-with-a-Multiresolution-Hash-Encoding" class="headerlink" title="Instant Neural Graphics Primitives with a Multiresolution Hash Encoding"></a>Instant Neural Graphics Primitives with a Multiresolution Hash Encoding</h1><p><a href="https://nvlabs.github.io/instant-ngp/">Instant Neural Graphics Primitives with a Multiresolution Hash Encoding (nvlabs.github.io)</a></p><h1 id="施工ing"><a href="#施工ing" class="headerlink" title="施工ing"></a>施工ing</h1>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;Instant-Neural-Graphics-Primitives-with-a-Multiresolution-Hash-Encoding&quot;&gt;&lt;a href=&quot;#Instant-Neural-Graphics-Primitives-with-a-Multire</summary>
      
    
    
    
    <category term="CV" scheme="http://example.com/categories/CV/"/>
    
    
    <category term="CV" scheme="http://example.com/tags/CV/"/>
    
    <category term="NeRF" scheme="http://example.com/tags/NeRF/"/>
    
    <category term="神经渲染" scheme="http://example.com/tags/%E7%A5%9E%E7%BB%8F%E6%B8%B2%E6%9F%93/"/>
    
  </entry>
  
  <entry>
    <title>NeRF</title>
    <link href="http://example.com/2024/04/25/nerf/"/>
    <id>http://example.com/2024/04/25/nerf/</id>
    <published>2024-04-25T07:12:45.000Z</published>
    <updated>2024-04-25T07:33:58.018Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Representing-Scenes-as-Neural-Radiance-Fields-for-View-Synthesis"><a href="#Representing-Scenes-as-Neural-Radiance-Fields-for-View-Synthesis" class="headerlink" title="Representing Scenes as Neural Radiance Fields for View Synthesis"></a>Representing Scenes as Neural Radiance Fields for View Synthesis</h1><p><a href="https://www.matthewtancik.com/nerf">NeRF: Neural Radiance Fields (matthewtancik.com)</a></p><h1 id="施工ing"><a href="#施工ing" class="headerlink" title="施工ing"></a>施工ing</h1>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;Representing-Scenes-as-Neural-Radiance-Fields-for-View-Synthesis&quot;&gt;&lt;a href=&quot;#Representing-Scenes-as-Neural-Radiance-Fields-for-View-S</summary>
      
    
    
    
    <category term="CV" scheme="http://example.com/categories/CV/"/>
    
    
    <category term="CV" scheme="http://example.com/tags/CV/"/>
    
    <category term="NeRF" scheme="http://example.com/tags/NeRF/"/>
    
    <category term="神经渲染" scheme="http://example.com/tags/%E7%A5%9E%E7%BB%8F%E6%B8%B2%E6%9F%93/"/>
    
  </entry>
  
  <entry>
    <title>games104-Lumen</title>
    <link href="http://example.com/2023/12/11/104-lumen/"/>
    <id>http://example.com/2023/12/11/104-lumen/</id>
    <published>2023-12-11T14:09:56.000Z</published>
    <updated>2024-04-25T07:21:12.639Z</updated>
    
    <content type="html"><![CDATA[<h3 id="传统功夫"><a href="#传统功夫" class="headerlink" title="传统功夫"></a>传统功夫</h3><hr><p><strong>蒙特卡洛积分Monte Carlo Integration</strong>，基于采样Sampling</p><p>优化：主要对采样的方式进行优化</p><p><strong>RSM</strong>：光子映射</p><p><strong>LPV</strong>：分块，光线在块内碰撞</p><p><strong>SVOGI</strong>：片元变体素</p><p><strong>VXGI</strong>：上面的优化，clipmap存储数据，近处voxel更细远处更粗</p><p><strong>SSGI</strong>：屏幕空间，可用于镜面效果等</p><h3 id="Lumen"><a href="#Lumen" class="headerlink" title="Lumen"></a>Lumen</h3><hr><p>基础：<strong>SDF</strong>，面片sdf，由于连续函数可微，导数就是法线方向，光线步进算法，阴影计算等都可以大大简化，一般分4层lumen，精度按照与摄像机的距离逐渐降低</p><p>核心：在表面附近放置大量probe</p><p>Mesh card：每个物体6个面，采样，各种信息（法线、深度等等）</p><p>surface cache：4096*4096，存储一系列mesh card</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h3 id=&quot;传统功夫&quot;&gt;&lt;a href=&quot;#传统功夫&quot; class=&quot;headerlink&quot; title=&quot;传统功夫&quot;&gt;&lt;/a&gt;传统功夫&lt;/h3&gt;&lt;hr&gt;
&lt;p&gt;&lt;strong&gt;蒙特卡洛积分Monte Carlo Integration&lt;/strong&gt;，基于采样Sampli</summary>
      
    
    
    
    <category term="游戏引擎" scheme="http://example.com/categories/%E6%B8%B8%E6%88%8F%E5%BC%95%E6%93%8E/"/>
    
    
    <category term="游戏引擎" scheme="http://example.com/tags/%E6%B8%B8%E6%88%8F%E5%BC%95%E6%93%8E/"/>
    
  </entry>
  
  <entry>
    <title>games104-gameplay和游戏AI</title>
    <link href="http://example.com/2023/11/24/104-ai/"/>
    <id>http://example.com/2023/11/24/104-ai/</id>
    <published>2023-11-24T13:42:09.000Z</published>
    <updated>2024-04-25T07:21:28.660Z</updated>
    
    <content type="html"><![CDATA[<h2 id="gameplay"><a href="#gameplay" class="headerlink" title="gameplay"></a>gameplay</h2><h3 id="事件机制"><a href="#事件机制" class="headerlink" title="事件机制"></a>事件机制</h3><hr><p><strong>发布订阅模式</strong></p><p>核心：</p><ul><li>事件定义：可以用面向对象的类定义，但是如何扩展、如何热重载是个很大问题，利用反射写在外部文件？dll注入？</li><li>回调函数注册：GO没了咋办，空指针问题，强引用（shared_ptr,有东西没释放对象也不能释放）&#x2F; 弱引用（常用一点，对某一时刻查看场景中其他对象可以用如weak_ptr）</li><li>消息分发：循环队列事件池，加上多个不同类型的队列架构，大量消息的触发导致帧延迟，如何解决？</li></ul><h3 id="脚本系统"><a href="#脚本系统" class="headerlink" title="脚本系统"></a>脚本系统</h3><hr><p><strong>游戏逻辑</strong>，热更新，编译型语言不行</p><p>解释型语言闪亮登场，方便热更新，寄了也是虚拟机寄不影响引擎，非常哇塞</p><p><strong>GO管理</strong>，利用解释性语言的垃圾回收！，适用于对象特别多的，简单的可以引擎内核直接管理</p><h3 id="可视化脚本"><a href="#可视化脚本" class="headerlink" title="可视化脚本"></a>可视化脚本</h3><hr><p><strong>blueprint蓝图！</strong>低代码平台</p><h2 id="基础AI"><a href="#基础AI" class="headerlink" title="基础AI"></a>基础AI</h2><h3 id="导航Navigation"><a href="#导航Navigation" class="headerlink" title="导航Navigation"></a>导航Navigation</h3><hr><ul><li>地图表示</li><li>寻路：<strong>A*算法</strong>：启发式算法，g(n)+f(n)已经走过的距离+估计要走的距离，优先搜索估计最近的点，同时找到目的就结束计算（区分与迪杰斯特拉算法）</li><li>路径平滑：<strong>Funnel算法</strong>，烟囱收缩大法，很强，直接直线走</li></ul><h3 id="转向"><a href="#转向" class="headerlink" title="转向"></a>转向</h3><hr><p>主要用于车辆寻路</p><h3 id="群体模拟"><a href="#群体模拟" class="headerlink" title="群体模拟"></a>群体模拟</h3><hr><ul><li>宏观定义：定义一个路线，让npc走</li><li>围观控制：定义个体，比如和周围物体靠太近就远离</li></ul><p>是否要为每个npc设置寻路？设置障碍的距离场，距离越近斥力越强，从而实现群体的控制</p><h3 id="环境感知"><a href="#环境感知" class="headerlink" title="环境感知"></a>环境感知</h3><hr><p>基于位置、血量、空间信息、战术地图、热力图、周围物体等等基础信息</p><h3 id="行为树"><a href="#行为树" class="headerlink" title="行为树"></a>行为树</h3><hr><p>对人类思考方式的一种模拟</p><p>一些中间节点</p><ul><li>Sequence：顺序执行所有子树，例如开门动作，描述为一个task序列，子节点返回是否完成，逻辑清晰</li><li>Selector：分支，完成一个就返回</li><li>Parallel：并行执行子树，例如实现npc的边走边开枪</li><li>Decorator：常用小东西抽象出来的节点</li></ul><p>每次tick都从根节点开始（为了加速可能直接激活树中的一些节点），同时running的节点可能有很多，一些行为是可以被打断的</p><p><strong>Blackboard</strong>：实现行为树中间的信息交换</p><h2 id="高级AI"><a href="#高级AI" class="headerlink" title="高级AI"></a>高级AI</h2><p>上述的都是forward的实现，有点死板，高级AI根据目标反向构建行为</p><h3 id="HTN（Hierarchical-Tasks-Network）"><a href="#HTN（Hierarchical-Tasks-Network）" class="headerlink" title="HTN（Hierarchical Tasks Network）"></a>HTN（Hierarchical Tasks Network）</h3><hr><p>层次任务网络，Dark Soul 3！Horizon！Dying Lighting 2!</p><h3 id="GOAP"><a href="#GOAP" class="headerlink" title="GOAP"></a>GOAP</h3><hr><p>目标导向行为规划，目标集、行动集</p><h3 id="MCTS蒙特卡洛树搜索"><a href="#MCTS蒙特卡洛树搜索" class="headerlink" title="MCTS蒙特卡洛树搜索"></a>MCTS蒙特卡洛树搜索</h3><hr><p>AlphaGO！</p><p>类似人类下围棋的思考，树形模拟棋局，根据大量棋谱作为默认策略，进行快速模拟和决策</p><p>成功率 &#x3D; Q&#x2F;N，Q:成功次数，N:从某个点出发做的模拟次数</p><ul><li>选择最有希望的节点</li><li>扩展树</li><li>模拟输赢</li><li>反向传播，向上更新节点成功率</li></ul><p>优先未探索的还是当前成功率较高的节点？根据是否保守、父节点模拟次数</p><p>如何选择下一步？很多策略，访问次数最多、成功率最高、采样节点最高、或者继续爆算等等</p><h3 id="机器学习"><a href="#机器学习" class="headerlink" title="机器学习"></a>机器学习</h3><hr><p>如何描述游戏世界？地图、态势表达</p><p>如何对游戏世界做些什么？攻击、移动等等</p><p>奖惩设计？输赢-1+1</p><p>网路结构设计？对各个输入应用不同的网络编码（ResNet、MLP、transformer、LSTM等等），再解码输出</p><p>训练策略？防止陷入局部最优，和多个对手互博</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;gameplay&quot;&gt;&lt;a href=&quot;#gameplay&quot; class=&quot;headerlink&quot; title=&quot;gameplay&quot;&gt;&lt;/a&gt;gameplay&lt;/h2&gt;&lt;h3 id=&quot;事件机制&quot;&gt;&lt;a href=&quot;#事件机制&quot; class=&quot;headerlink&quot; </summary>
      
    
    
    
    <category term="游戏引擎" scheme="http://example.com/categories/%E6%B8%B8%E6%88%8F%E5%BC%95%E6%93%8E/"/>
    
    
    <category term="游戏引擎" scheme="http://example.com/tags/%E6%B8%B8%E6%88%8F%E5%BC%95%E6%93%8E/"/>
    
  </entry>
  
  <entry>
    <title>games104-动画、物理、粒子系统</title>
    <link href="http://example.com/2023/11/21/104-animation/"/>
    <id>http://example.com/2023/11/21/104-animation/</id>
    <published>2023-11-21T13:59:15.000Z</published>
    <updated>2024-04-25T07:21:20.310Z</updated>
    
    <content type="html"><![CDATA[<h2 id="动画"><a href="#动画" class="headerlink" title="动画"></a>动画</h2><h3 id="概览"><a href="#概览" class="headerlink" title="概览"></a>概览</h3><hr><p><strong>一些挑战</strong></p><ul><li>动作不可预测，如何realtime、如何对适配复杂的环境交互</li><li>表情细节</li><li>不同部位的动作如何融合</li></ul><p><strong>3D动画</strong></p><hr><p><strong>DoF</strong>：Degree of freedom，自由度，刚体6个自由度，三维中的平移和旋转 3+3</p><p>几种实现方式：</p><ul><li>早期刚体动画：方块，连接处不自然</li><li>顶点动画：可模拟布料、流体</li><li>Morph Animation（顶点影响权重，插值）：一般用于面部动作</li><li>Skinned Animation（蒙皮动画）：目前常用</li><li>基于物理：布娃娃Ragdoll、布料、流体、反向动力IK，有定点如何自然运动</li></ul><h3 id="蒙皮动画"><a href="#蒙皮动画" class="headerlink" title="蒙皮动画"></a>蒙皮动画</h3><hr><p>mesh、skeleton（分为Joint、Bone）、刷蒙皮（顶点绑定骨骼）、骨骼动画</p><p>骨骼可以拓展至衣服、外饰、面部，武器（可以插在手部joint，插槽的概念），但最好定义好基础骨骼</p><p>Root joint，一般在模型最底层的中间位置，方便高度等位置表达</p><h3 id="数学基础-旋转"><a href="#数学基础-旋转" class="headerlink" title="数学基础-旋转"></a>数学基础-旋转</h3><hr><p>三维空间中的旋转可以通过<strong>三个轴的旋转</strong>叠加出来，欧拉角，一般用于编辑物体，但是</p><ul><li>当顺序不同时结果可能不同</li><li>难插值</li><li>难用于定轴旋转</li></ul><hr><p><strong>四元数！</strong></p><p>引入，复平面下的角度叠加可以用复数的相乘表示</p><p>群论定义在三维空间下的计算</p><p>定义了三个虚部 i、j、k，Q &#x3D; a+b*i+c*j+d*k</p><p>将旋转优化为直接的矩阵计算</p><iframe src="//player.bilibili.com/player.html?aid=33385105&bvid=BV1SW411y7W1&cid=58437850&p=1" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true"> </iframe><h3 id="动画压缩"><a href="#动画压缩" class="headerlink" title="动画压缩"></a>动画压缩</h3><hr><p>缩放、等不旋转等骨骼直接不存</p><p>利用关键帧插值，<strong>Catmull曲线</strong>，非常光滑插值很不错，四元数浮点数定点压缩</p><h2 id="物理系统"><a href="#物理系统" class="headerlink" title="物理系统"></a>物理系统</h2><p>要长脑子了，由于都是物理细节，所以决定跳过！</p><h3 id="Actor-Shape"><a href="#Actor-Shape" class="headerlink" title="Actor  Shape"></a>Actor  Shape</h3><hr><ul><li><p>球体：球体、小型物体</p></li><li><p>胶囊体：人物</p></li><li><p>方块：</p></li><li><p>凸包：</p></li><li><p>三角面片：静态物体</p></li><li><p>Height Field：用于地形</p></li></ul><h3 id="物理材质"><a href="#物理材质" class="headerlink" title="物理材质"></a>物理材质</h3><hr><p>区别于PBR的光学特性，物理材质表示的是摩擦力、弹性等等</p><h3 id="碰撞检测"><a href="#碰撞检测" class="headerlink" title="碰撞检测"></a>碰撞检测</h3><hr><p>AABB初筛，选出可能碰撞的物体，再细选BVH或sort and sweep(轴排序)</p><p><strong>Minkowki Sum</strong>：太抽象了，大概是集合上定义了一个域，结果就是两个碰撞凸包形体的Minkowki和形成的多面体必过原点</p><h3 id="碰撞处理"><a href="#碰撞处理" class="headerlink" title="碰撞处理"></a>碰撞处理</h3><hr><p>普通处理直接给力，容易起飞</p><p>拉格朗日约束，还是很抽象，通过一个个小冲量不断叠加求拉格朗日约束得到合适的反冲力</p><h3 id="模拟的不稳定"><a href="#模拟的不稳定" class="headerlink" title="模拟的不稳定"></a>模拟的不稳定</h3><hr><p>不同显卡、不同检测条件，两台电脑对同样的输入模拟不同</p><h3 id="角色控制"><a href="#角色控制" class="headerlink" title="角色控制"></a>角色控制</h3><hr><p>一个反物理系统</p><p>一般用胶囊形状，楼梯、斜坡、各种姿态碰撞盒的表示、和周围物体如何交互、摄像机位置等等都需要处理</p><h3 id="布娃娃系统"><a href="#布娃娃系统" class="headerlink" title="布娃娃系统"></a>布娃娃系统</h3><hr><p>利用关键骨骼挂ragdoll，但要设计正常的joint的约束，一般会物理效果加ragdoll一起用</p><h3 id="布料"><a href="#布料" class="headerlink" title="布料"></a>布料</h3><hr><ul><li><p>利用骨骼，移动端常用</p></li><li><p>ragdoll，动力学模拟</p></li><li><p>mesh-based，物理解算，弹簧质点模型</p><p>自穿插问题</p></li></ul><h2 id="粒子系统"><a href="#粒子系统" class="headerlink" title="粒子系统"></a>粒子系统</h2><h3 id="渲染"><a href="#渲染" class="headerlink" title="渲染"></a>渲染</h3><hr><p>透明物渲染问题，排序问题</p><h3 id="声音"><a href="#声音" class="headerlink" title="声音"></a>声音</h3><hr><p>声音的采样、编码、压缩</p><p>声学渲染</p><p>多普勒效应模拟</p><p>空间感实现</p><p>回声实现</p><p>常用中间件：<strong>fmod、wwise</strong></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;动画&quot;&gt;&lt;a href=&quot;#动画&quot; class=&quot;headerlink&quot; title=&quot;动画&quot;&gt;&lt;/a&gt;动画&lt;/h2&gt;&lt;h3 id=&quot;概览&quot;&gt;&lt;a href=&quot;#概览&quot; class=&quot;headerlink&quot; title=&quot;概览&quot;&gt;&lt;/a&gt;概览&lt;/h3&gt;&lt;hr&gt;
&lt;</summary>
      
    
    
    
    <category term="游戏引擎" scheme="http://example.com/categories/%E6%B8%B8%E6%88%8F%E5%BC%95%E6%93%8E/"/>
    
    
    <category term="游戏引擎" scheme="http://example.com/tags/%E6%B8%B8%E6%88%8F%E5%BC%95%E6%93%8E/"/>
    
  </entry>
  
  <entry>
    <title>games104笔记-开篇和渲染</title>
    <link href="http://example.com/2023/11/18/games104/"/>
    <id>http://example.com/2023/11/18/games104/</id>
    <published>2023-11-18T14:35:39.000Z</published>
    <updated>2024-04-25T07:21:57.508Z</updated>
    
    <content type="html"><![CDATA[<h3 id="简化的架构"><a href="#简化的架构" class="headerlink" title="简化的架构"></a>简化的架构</h3><hr><ul><li><strong>Tool Layer</strong>：面向用户，DCC和引擎交互</li><li><strong>Function Layer</strong>：tick，渲染+逻辑，etc…</li><li><strong>Resource Layer</strong>：资源格式转换为资产（更高效的内部数据），GUID资产识别，资产生命周期管理（核心）</li><li><strong>Core Layer</strong>：数学库，更高效的数据结构、容器，内存管理</li><li><strong>Platform Layer</strong>：RHI（封装不同的图形API，向上提供统一接口）</li></ul><p>Function 和 Core 可能有很多交汇</p><h3 id="Game-World"><a href="#Game-World" class="headerlink" title="Game World"></a>Game World</h3><hr><p>各种光源、人物角色、像机乃至空气墙、触发器都可以表示成<strong>对象Object</strong></p><p>各种过于复杂的关系导致继承系统很难表示，引入<strong>组件components</strong>，可以看一下ECS模式</p><p><strong>component-based Tick</strong>，相同组件同时tick，更加高效</p><p><strong>事件系统</strong>，消息机制类似进程信箱通信，需要可扩展消息以支持物体间各种交互事件</p><p><strong>场景管理</strong>，八叉树、BVH（bounding box）等等</p><p><strong>pretick &amp; posttick</strong>，多线程下统一时序</p><h3 id="引擎中的渲染"><a href="#引擎中的渲染" class="headerlink" title="引擎中的渲染"></a>引擎中的渲染</h3><hr><p>可结合games101</p><p><strong>一些挑战</strong></p><ul><li>场景中的大量物体需要渲染</li><li>复杂的底层硬件适配</li><li>稳定的帧率</li><li>有限的资源分配</li></ul><p>GPU，SIMT结构，GPU架构</p><p>mash - submash - shader，可以按照材质排序渲染，<strong>实例Instance</strong>概念（定义和渲染概念分离）</p><p><strong>可见性裁剪</strong>，包围盒+场景划分（BVH）</p><p><strong>纹理压缩</strong>，常用压缩算法无法随机访问位置，使用基于Block Compression的常用算法</p><h3 id="光、材质和shader"><a href="#光、材质和shader" class="headerlink" title="光、材质和shader"></a>光、材质和shader</h3><hr><p>再次参考101</p><p><strong>一些挑战</strong></p><ul><li>BRDF如何快速进行积分</li><li>光源本身的复杂性、间接光源的处理</li><li>shadow！</li></ul><p><strong>预计算全局光照（Global IIIumination）</strong>，球面空间如何采样和积分，球谐函数（Spherical Harmonics）！！！</p><p>个人对使用SH函数的理解，不需要采样球上成千上万的点，而且全局光关注的基本是低频信息，直接使用类似傅里叶变换从空域到频域映射加卷积提取其特征即可，球谐基本就是这个作用，可以非常高效的提取亮部和暗部，可以看一下各级球谐函数的图像，阶数越高，能表示的精度越高（亮暗面更多），12个参数（球谐函数4个*RGB3个）即可表示</p><p><strong>SH Lightmap</strong>：烘培很慢但效果很好，基本适用于静态物体</p><p><strong>Light Probe &amp;&amp; Reflect Probe</strong>：低频采样但是可以实时</p><p><strong>PBR</strong>：当前常用的有</p><ul><li><p>Specular Glossiness（SG模型，RGB+菲涅尔+粗糙度，DFG！），菲涅尔项容易炸</p></li><li><p>Metallic Roughness（RGB+粗糙度+金属度），根据金属度设置菲涅尔项，更强的可控性</p></li></ul><p><strong>IBL</strong>：Image-based Lighting，会有多级表示不同的频率</p><p><strong>VSSM</strong>：利用方差、期望等直接求软阴影</p><hr><p><strong>总结-引擎渲染部分实现</strong></p><p>全局光照 -&gt; SH lightmap + Probe</p><p>材质 -&gt; PBR两大模型 + IBL</p><p>阴影 -&gt; VSSM + Cascade</p><h3 id="地形、植被（terrain，vegetation）"><a href="#地形、植被（terrain，vegetation）" class="headerlink" title="地形、植被（terrain，vegetation）"></a>地形、植被（terrain，vegetation）</h3><hr><p><strong>LOD</strong>：level of detail，层次细节</p><p><strong>QuadTree-Based Subdivision</strong>：四叉树地形管理</p><p>地形纹理交界处混合，hack，加bias！</p><p><strong>Virtual texture</strong>：类似mipmap，混合全部材质，有点像OS里的页面置换，存显存里！</p><p>核心思路就是近处细分，远处粗糙</p><h3 id="大气sky"><a href="#大气sky" class="headerlink" title="大气sky"></a>大气sky</h3><hr><p>首先，看到对的就是对的，<strong>拟合大气表现</strong>！两个参数：视线和太阳、视线和天顶的角度，但只能有一种表现，没有天气，只能在地面…</p><p>进一步，<strong>不是真空</strong>，光会穿过空中的气溶胶、水汽等等，会发生吸收、散射、气体自发光、来自其他物体的光，积分全是积分！从物体一路积分过来，有多少光能穿过来（通透度）、其他光对这条光路的贡献</p><p>再进一步，对<strong>瑞利散射</strong>的模拟，<strong>米氏散射</strong>的模拟</p><p>具体实现，</p><p>基于ray marching，沿着一条光路采样、积分得到结果，可以预渲染precomputed，存到贴图中，得到角度等参数时直接用即可</p><h3 id="云cloud"><a href="#云cloud" class="headerlink" title="云cloud"></a>云cloud</h3><hr><p>mesh云：加噪声等等</p><p>billboard云：二维片模拟</p><p>体积云：实时处理，weather texture很有用</p><p>噪声立大功</p><h3 id="环境光遮蔽AO"><a href="#环境光遮蔽AO" class="headerlink" title="环境光遮蔽AO"></a>环境光遮蔽AO</h3><hr><p>几种实现</p><ul><li>离线预计算，烘培到贴图</li><li>SSAO+，但屏幕空间无法处理不同物体距离</li><li>HBAO，看天顶，多少角度可以越过周围遮挡物，但无法区分不同角度照过来的光</li><li>GTAO，区分了光的角度，无敌</li></ul><h3 id="雾"><a href="#雾" class="headerlink" title="雾"></a>雾</h3><hr><p>几种实现</p><ul><li>深度雾，通过线性函数</li><li>height fog，雾会沉在下面，根据高度展示，积分浓度</li><li>体素雾，类似云渲染</li></ul><h3 id="后处理Post-process"><a href="#后处理Post-process" class="headerlink" title="后处理Post-process"></a>后处理Post-process</h3><hr><p>主要应用</p><ul><li><strong>Bloom效果</strong>：泛光，提取亮部再blur，会对结果缩小再放大，为了得到晕开的效果</li><li><strong>Tone Mapping</strong>：色调映射，很多映射曲线，经典<strong>filmic curve</strong>，目前用的多的是<strong>ACES</strong></li><li><strong>color grading</strong>：调色，又叫LUT，也是一个简单的映射</li></ul><h3 id="渲染管线pipeline"><a href="#渲染管线pipeline" class="headerlink" title="渲染管线pipeline"></a>渲染管线pipeline</h3><hr><p><strong>前向渲染forward rendering</strong></p><p>上面的n种渲染效果，大量算法，按照一定的顺序应用和处理，最终呈现</p><p>有方向性的数据流</p><p><strong>延迟渲染deferred rendering</strong></p><p>随着场景愈来愈复杂光源越来越多，更好的处理光源</p><p>先走一遍管线，把材质、深度、法线等信息全部存到<strong>GBuffer</strong>里，<strong>最后计算光照</strong></p><p>但是GBuffer开销极大</p><p><strong>Tile-based rendering</strong></p><p>目前比较常用，像瓷砖一块一块</p><p>相当于分割成几个小块，可以单独计算GBuffer、光照，大大减少开销</p><p><strong>Cluster-based rendering</strong></p><p>直接对视锥空间切分，四棱柱，tile的升级版本，更大量的光源</p><p><strong>几何buffer和材质buffer分离</strong></p><p>GBuffer、VBuffer分离</p><p>对buffer直接渲染！！！</p><h3 id="各种sync"><a href="#各种sync" class="headerlink" title="各种sync"></a>各种sync</h3><hr><p>保证frame buffer写完再刷，保证显示不会裂开</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h3 id=&quot;简化的架构&quot;&gt;&lt;a href=&quot;#简化的架构&quot; class=&quot;headerlink&quot; title=&quot;简化的架构&quot;&gt;&lt;/a&gt;简化的架构&lt;/h3&gt;&lt;hr&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Tool Layer&lt;/strong&gt;：面向用户，DCC和引擎交互&lt;/li&gt;
</summary>
      
    
    
    
    <category term="游戏引擎" scheme="http://example.com/categories/%E6%B8%B8%E6%88%8F%E5%BC%95%E6%93%8E/"/>
    
    
    <category term="游戏引擎" scheme="http://example.com/tags/%E6%B8%B8%E6%88%8F%E5%BC%95%E6%93%8E/"/>
    
  </entry>
  
  <entry>
    <title>docker</title>
    <link href="http://example.com/2023/11/10/docker/"/>
    <id>http://example.com/2023/11/10/docker/</id>
    <published>2023-11-10T11:16:54.000Z</published>
    <updated>2023-11-10T15:09:39.523Z</updated>
    
    <content type="html"><![CDATA[<p>docker官方一个很好的入门教程<a href="https://docs.docker.com/get-started/">Overview of the get started guide | Docker Docs</a></p><h2 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h2><hr><p>A container is a <strong>sandboxed process</strong> running on a host machine that is <strong>isolated from all other processes</strong> running on that host machine. That isolation leverages <strong>kernel namespaces and cgroups</strong>.</p><p>如果对OS熟悉的话，可以类比成一个从镜像文件构建的文件系统（多了隔离性）</p><h2 id="Core"><a href="#Core" class="headerlink" title="Core"></a>Core</h2><hr><p>核心只有 <strong>container</strong> 和 <strong>image</strong> </p><p>简单来说，容器是镜像的一个实例，修改容器再把内容全部写入镜像，就像一个U盘，不管放到哪台电脑一插即用</p><p>就ubuntu镜像来说，可以理解为把其iso文件pull下来运行，你可以在ubuntu容器里各种操作，退出时把数据写入iso文件（对于应用源文件来说）</p><p>所谓的用户数据容器是只在临时空间存储的，如果直接放入image中会产生严重的安全问题（普通数据通过挂载等方式，仅在本地使用）</p><h2 id="Important-Conception"><a href="#Important-Conception" class="headerlink" title="Important Conception"></a>Important Conception</h2><hr><h3 id="fundamental"><a href="#fundamental" class="headerlink" title="fundamental"></a>fundamental</h3><hr><p>container &amp; image &amp; docker workflow</p><p><code>Dockerfile</code>：构建镜像的文本文件，<code>container --&gt; image</code></p><h3 id="storage"><a href="#storage" class="headerlink" title="storage?"></a>storage?</h3><hr><p>一般情况下，容器只操作一个临时空间，数据无法持久化存储</p><p>一个基本解决方式，利用宿主机的磁盘，将数据文件挂载上去，下一次容器实例化时就可以从之前的位置继续操作</p><p><code>docker volume create &lt;datasetfile&gt;</code></p><p><code>docker run -dp &lt;HOST:CONTAINER&gt; --mount type=volume,src=&lt;datasetfile&gt;,target=&lt;path/to/file&gt; &lt;image-name&gt;</code></p><p>you can learn how to bind mount：<a href="https://docs.docker.com/get-started/06_bind_mounts/">使用绑定挂载 |Docker 文档</a>，more flexble.</p><h3 id="Multi-Container"><a href="#Multi-Container" class="headerlink" title="Multi-Container"></a>Multi-Container</h3><hr><p>通过构建容器网络使容器间可以进行交互</p><h2 id="Commands"><a href="#Commands" class="headerlink" title="Commands"></a>Commands</h2><hr><h3 id="common"><a href="#common" class="headerlink" title="common"></a>common</h3><p><code>docker ps</code>：show containers in use</p><p><code>docker image ls</code>：list your images</p><p><code>docker stop &lt;the-container-id&gt;</code>：stop container</p><p><code>docker rm &lt;the-container-id&gt;</code>：remove container</p><p><code>docker build -t getting-started ./</code>：create image by using Dockerfile，<code>-t</code> is tag your image name ，<code>./</code>is the path of Dockerfile</p><p><code>docker run -dp &lt;HOST:CONTAINER&gt; &lt;image-name&gt;</code>：run your container from image</p><h3 id="share"><a href="#share" class="headerlink" title="share"></a>share</h3><p><code>docker login</code>：sign in</p><p><code>docker tag &lt;LOCAL-IMAGE-NAME&gt; &lt;YOUR-USER-NAME/YOUR-IMAGE-NAME&gt;</code>：rename image</p><p><code>docker push YOUR-USER-NAME/YOUR-IMAGE-NAME:tagname</code>：push to your repositories</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;docker官方一个很好的入门教程&lt;a href=&quot;https://docs.docker.com/get-started/&quot;&gt;Overview of the get started guide | Docker Docs&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&quot;Overview&quot;</summary>
      
    
    
    
    <category term="技术" scheme="http://example.com/categories/%E6%8A%80%E6%9C%AF/"/>
    
    
    <category term="docker" scheme="http://example.com/tags/docker/"/>
    
  </entry>
  
  <entry>
    <title>cmake</title>
    <link href="http://example.com/2023/11/06/cmake/"/>
    <id>http://example.com/2023/11/06/cmake/</id>
    <published>2023-11-06T09:41:52.000Z</published>
    <updated>2023-11-07T14:02:44.613Z</updated>
    
    <content type="html"><![CDATA[<p>cmake的小记录，最近在Linux上整活，正好学一下</p><p>暂时不涉及库文件的生成</p><h3 id="文件树"><a href="#文件树" class="headerlink" title="文件树"></a>文件树</h3><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 项目标准文件树</span></span><br><span class="line">.</span><br><span class="line">├── CMakeLists.txt</span><br><span class="line">├── include</span><br><span class="line">├── linux.sh</span><br><span class="line">└── src</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># cmake后的文件树，可执行文件在bin中，构建在build中</span></span><br><span class="line">.</span><br><span class="line">├── CMakeLists.txt</span><br><span class="line">├── bin</span><br><span class="line">├── build</span><br><span class="line">├── include</span><br><span class="line">├── linux.sh</span><br><span class="line">└── src</span><br></pre></td></tr></table></figure><h3 id="CMakeLists-txt"><a href="#CMakeLists-txt" class="headerlink" title="CMakeLists.txt"></a>CMakeLists.txt</h3><hr><h4 id="CMakeLists"><a href="#CMakeLists" class="headerlink" title="CMakeLists"></a>CMakeLists</h4><p>感觉CMakeLists放最顶层方便一点，当然模块化的话就通过<code>add_subdirectory</code>递归地构建，基本满足目前的99%需要</p><figure class="highlight cmake"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">cmake_minimum_required</span>(VERSION <span class="number">3.15</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># set the project name</span></span><br><span class="line"><span class="keyword">project</span>(<span class="keyword">test</span>-cmake)</span><br><span class="line"></span><br><span class="line"><span class="comment"># bin file output</span></span><br><span class="line"><span class="comment"># you can also set others </span></span><br><span class="line"><span class="keyword">set</span>(EXECUTABLE_OUTPUT_PATH <span class="variable">$&#123;PROJECT_SOURCE_DIR&#125;</span>/bin)</span><br><span class="line"></span><br><span class="line"><span class="comment"># include file path</span></span><br><span class="line"><span class="keyword">include_directories</span>(<span class="variable">$&#123;PROJECT_SOURCE_DIR&#125;</span>/<span class="keyword">include</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># your source files</span></span><br><span class="line"><span class="comment"># you can also use </span></span><br><span class="line"><span class="comment"># set(SRC_LIST ./src/main.cpp ./src/util.cpp ./src/nums.cpp)</span></span><br><span class="line"><span class="keyword">aux_source_directory</span>(src SRC_LIST)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span>(UNIX)</span><br><span class="line"><span class="keyword">MESSAGE</span>(<span class="string">&quot;\nThis is Linux.\n&quot;</span>)</span><br><span class="line"><span class="comment"># you can set some config here. </span></span><br><span class="line"><span class="comment"># add_compile_options(-std=c++17 -Wall) </span></span><br><span class="line"><span class="keyword">elseif</span>(WIN32)</span><br><span class="line"><span class="keyword">MESSAGE</span>(<span class="string">&quot;This is Windows.&quot;</span>)</span><br><span class="line"><span class="keyword">endif</span>()</span><br><span class="line"></span><br><span class="line"><span class="comment"># compile_options</span></span><br><span class="line"><span class="comment"># for your gcc/g++</span></span><br><span class="line"><span class="keyword">add_compile_options</span>(-std=c++<span class="number">17</span> -O2 -lpthread -Wall)</span><br><span class="line"></span><br><span class="line"><span class="comment"># add the executable</span></span><br><span class="line"><span class="keyword">add_executable</span>(<span class="keyword">test</span>-cmake <span class="variable">$&#123;SRC_LIST&#125;</span>)</span><br></pre></td></tr></table></figure><p>有了 CMakeLists.txt 就可以直接在根目录下执行 bash，完成构建和编译，以下给出脚本文件</p><h4 id="Linux-Bash"><a href="#Linux-Bash" class="headerlink" title="Linux Bash"></a>Linux Bash</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">mkdir</span> ./build</span><br><span class="line"><span class="built_in">cd</span> ./build</span><br><span class="line">cmake ..</span><br><span class="line">cmake --build . <span class="comment"># 统一的接口,不管底层是啥,直接--build即可,不然指明make或ninja等底层</span></span><br></pre></td></tr></table></figure><h4 id="Windows-Bat"><a href="#Windows-Bat" class="headerlink" title="Windows Bat"></a>Windows Bat</h4><figure class="highlight bat"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">md</span> build</span><br><span class="line"><span class="built_in">cd</span> .\build</span><br><span class="line">cmake ..</span><br><span class="line">cmake --build .</span><br></pre></td></tr></table></figure><h3 id="链接第三方库文件"><a href="#链接第三方库文件" class="headerlink" title="链接第三方库文件"></a>链接第三方库文件</h3><hr><ul><li>头文件 - 开发时</li><li>静态库.lib - 编译时</li><li>动态库.dll - 运行时，不会！大佬教教！</li></ul><figure class="highlight cmake"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 批量引入库文件和头文件</span></span><br><span class="line"><span class="comment"># REQUIRED：找不到库就报错</span></span><br><span class="line"><span class="comment"># COMPONENTS：从库中找子库（模块）xx，比如COMPONENTS Widget表示找到子模块Widget</span></span><br><span class="line"><span class="keyword">find_package</span>(OpenCV REQUIRED)</span><br><span class="line"><span class="keyword">find_package</span>(Qt5 COMPONENTS Core REQUIRED)</span><br><span class="line"></span><br><span class="line"><span class="comment"># OpenCV_INCLUDE_DIRS 是预定义变量，代表OpenCV库的头文件路径</span></span><br><span class="line"><span class="keyword">include_directories</span>(<span class="variable">$&#123;OpenCV_INCLUDE_DIRS&#125;</span>) </span><br><span class="line"> </span><br><span class="line"><span class="comment"># OpenCV_LIBS 是预定义变量，代表OpenCV库的lib库文件</span></span><br><span class="line"><span class="keyword">target_link_libraries</span>(YOUR_TARGET_NAME <span class="variable">$&#123;OpenCV_LIBS&#125;</span>)</span><br></pre></td></tr></table></figure><h3 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h3><hr><ul><li><p><a href="https://blog.csdn.net/challenglistic/article/details/129093434">【cmake学习】cmake 引入第三方库（头文件目录、库目录、库文件）_cmake include_directories_仲夏夜之梦~的博客-CSDN博客</a></p></li><li><p><a href="https://blog.csdn.net/whahu1989/article/details/82078563">Linux下CMake简明教程_linux cmake-CSDN博客</a></p></li><li><p><a href="https://cmake.org/documentation/">CMake Documentation and Community</a></p></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;cmake的小记录，最近在Linux上整活，正好学一下&lt;/p&gt;
&lt;p&gt;暂时不涉及库文件的生成&lt;/p&gt;
&lt;h3 id=&quot;文件树&quot;&gt;&lt;a href=&quot;#文件树&quot; class=&quot;headerlink&quot; title=&quot;文件树&quot;&gt;&lt;/a&gt;文件树&lt;/h3&gt;&lt;hr&gt;
&lt;figure cl</summary>
      
    
    
    
    <category term="技术" scheme="http://example.com/categories/%E6%8A%80%E6%9C%AF/"/>
    
    
    <category term="Linux" scheme="http://example.com/tags/Linux/"/>
    
    <category term="CMake" scheme="http://example.com/tags/CMake/"/>
    
  </entry>
  
  <entry>
    <title>WSL2+miniconda+Jupyter notebook</title>
    <link href="http://example.com/2023/10/31/conda-env/"/>
    <id>http://example.com/2023/10/31/conda-env/</id>
    <published>2023-10-31T03:05:00.000Z</published>
    <updated>2023-10-31T04:25:45.437Z</updated>
    
    <content type="html"><![CDATA[<h1 id="WSL2-miniconda-Jupyter-notebook"><a href="#WSL2-miniconda-Jupyter-notebook" class="headerlink" title="WSL2+miniconda+Jupyter notebook"></a>WSL2+miniconda+Jupyter notebook</h1><p>win11下构建的开发环境，顺便熟悉一下Linux下的开发</p><p>cuda、cuDNN可以直接在win下处理，会直接集成到WSL2，可用<code>nvidia-smi</code>查看相关信息</p><h2 id="WSL2安装"><a href="#WSL2安装" class="headerlink" title="WSL2安装"></a>WSL2安装</h2><hr><p><a href="https://learn.microsoft.com/zh-cn/windows/wsl/install">安装 WSL | Microsoft Learn</a></p><h2 id="minicond安装"><a href="#minicond安装" class="headerlink" title="minicond安装"></a>minicond安装</h2><hr><p><a href="https://docs.conda.io/projects/miniconda/en/latest/">Miniconda — miniconda 文档</a></p><p>conda包含了包管理和环境管理，很方便</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 免不了 conda 换源</span></span><br><span class="line">conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/msys2/</span><br><span class="line">conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/conda-forge</span><br><span class="line">conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/</span><br><span class="line">conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/pytorch/</span><br><span class="line">conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main/</span><br><span class="line">conda config --<span class="built_in">set</span> show_channel_urls <span class="built_in">yes</span></span><br></pre></td></tr></table></figure><h2 id="Jupyter"><a href="#Jupyter" class="headerlink" title="Jupyter"></a>Jupyter</h2><hr><p><code>conda create -n gpu-py3.10 python=3.10</code> 创建新环境</p><p><code>conda env list</code> 查看环境列表</p><p><code>conda activate gpu-py3.10</code> 激活环境</p><p><code>conda install jupyter notebook</code> 下载jupyter</p><p><code>jupyter notebook</code> 启动jupyter，win下可直接用浏览器访问其界面</p><h2 id="一些包的安装"><a href="#一些包的安装" class="headerlink" title="一些包的安装"></a>一些包的安装</h2><hr><p><a href="https://pytorch.org/">PyTorch</a>（注意cuda还是cpu版，很多时候用不了cuda可能是pytorch包弄错了）</p><p><a href="https://github.com/scikit-learn/scikit-learn">scikit-learn</a>:<code>conda install -c conda-forge scikit-learn</code></p><p>都用包管理器来处理即可，装的时候在哪个环境下别弄错，<code>pip</code>或者<code>conda</code>都行</p><h2 id="wsl2走代理"><a href="#wsl2走代理" class="headerlink" title="wsl2走代理"></a>wsl2走代理</h2><hr><p>clash最简单的一个方式</p><p>Service Mode + TUN Mode</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;WSL2-miniconda-Jupyter-notebook&quot;&gt;&lt;a href=&quot;#WSL2-miniconda-Jupyter-notebook&quot; class=&quot;headerlink&quot; title=&quot;WSL2+miniconda+Jupyter noteboo</summary>
      
    
    
    
    <category term="技术" scheme="http://example.com/categories/%E6%8A%80%E6%9C%AF/"/>
    
    
    <category term="Linux" scheme="http://example.com/tags/Linux/"/>
    
    <category term="ML" scheme="http://example.com/tags/ML/"/>
    
  </entry>
  
  <entry>
    <title>Linux拾遗</title>
    <link href="http://example.com/2023/10/25/Linux%E6%8B%BE%E9%81%97/"/>
    <id>http://example.com/2023/10/25/Linux%E6%8B%BE%E9%81%97/</id>
    <published>2023-10-25T15:09:00.000Z</published>
    <updated>2023-10-25T15:23:14.807Z</updated>
    
    <content type="html"><![CDATA[<p>回顾了以前学的内容，纯纯的只学命令而没有真的在linux下整过什么东西，恶补有关内容，真正拥抱这个社区</p><p>最近重新把 win+wsl2 拿起来，发现 nvidia 的 cuda 在 win 下安装后通过某种方式直接集成到wsl中了，vscode也接入了wsl，准备日后学习和工作迁移到linux上</p><h1 id="拾遗"><a href="#拾遗" class="headerlink" title="拾遗"></a>拾遗</h1><h2 id="GNU计划"><a href="#GNU计划" class="headerlink" title="GNU计划"></a>GNU计划</h2><hr><p><a href="https://zh.wikipedia.org/wiki/GNU">GNU - 维基百科</a></p><p>名称是GNU’s Not Unix!的缩写</p><p>Linux操作系统包涵了Linux内核与其他自由软件项目中的GNU组件和软件，可以被称为GNU&#x2F;Linux</p><p>其内容软件完全以GPL方式发布</p><h2 id="POSIX标准"><a href="#POSIX标准" class="headerlink" title="POSIX标准"></a>POSIX标准</h2><hr><p>可移植操作系统接口（Portable Operating System Interface of UNIX）</p><p>定义了应用程序和操作系统之间的接口，以及命令行shell和程序接口，以实现软件的可移植性</p><p>常见的Linux, BSD, macOS, iOS, Android等操作系统均遵循POSIX</p><h2 id="开源许可证"><a href="#开源许可证" class="headerlink" title="开源许可证"></a>开源许可证</h2><hr><ul><li>宽松式：BSD、MIT、Apache2</li><li>Copyleft：GPL（传染性）、LGPL、AGPL、Mozilla（MPL）</li></ul><p>选择时可以看那张经典的树状图，宽松式基本都可以，特别注意GPL的传染性</p><h2 id="万物皆文件"><a href="#万物皆文件" class="headerlink" title="万物皆文件"></a>万物皆文件</h2><hr><p>基本哲学之一</p><p>简单理解就是硬件设备、进程信息、网络接口，还是普通文件、目录、链接等，都可以通过文件的方式进行访问和操作，仅需要使用一套 API 和开发工具即可调取 Linux 系统中绝大部分的资源</p><h2 id="基本目录规范-XDG"><a href="#基本目录规范-XDG" class="headerlink" title="基本目录规范 - XDG"></a>基本目录规范 - XDG</h2><hr><p>为系统提供一致的桌面环境</p><p>XDG规范涵盖了各个方面的规范，使得不同的桌面环境和应用程序能够更好地协作和兼容</p><p>主要通过一套<strong>环境变量</strong>来指明应用程序的基准目录</p><p>通过<code>env | grep XDG</code>查看有关环境变量</p><p><a href="https://winddoing.github.io/post/ef694e1f.html">Linux 基本目录规范 ——XDG | Winddoing’s Notes</a></p><p><a href="https://specifications.freedesktop.org/basedir-spec/basedir-spec-latest.html">XDG Base Directory Specification (specifications.freedesktop.org)</a></p><h2 id="各目录作用"><a href="#各目录作用" class="headerlink" title="各目录作用"></a>各目录作用</h2><hr><p><a href="https://www.cnblogs.com/zhuchenglin/p/8686924.html">Linux各目录及每个目录的详细介绍 - lin_zone - 博客园 (cnblogs.com)</a></p><p>常用目录</p><ul><li><code>/bin -&gt; /usr/bin</code>：常用命令</li><li><code>/home</code>：用户主目录</li><li><code>/dev</code>：设备相关</li><li><code>/etc</code>：系统管理配置，如软件包源就在此处配置，谨慎修改</li><li><code>/lib -&gt; /usr/lib</code>：各种库</li><li><code>/proc</code>：VFS，内存的映射，可以用于查看系统信息</li><li><code>/opt</code>：一般用于大型软件和服务程序</li><li><code>/mnt</code>：临时挂载点</li><li><code>/usr</code>：主要存放应用程序和各种文件</li><li><code>/var</code>：各种不断扩充的文件，包括日志，例<code>cat /var/log/apt/term.log</code></li></ul><p>除了基本的目录作用，进一步了解各目录内部的信息，使用起来才能达到得心应手</p><h2 id="包管理"><a href="#包管理" class="headerlink" title="包管理"></a>包管理</h2><hr><p>理解包管理的C&#x2F;S模式，中心化机制</p><p><a href="https://linux.cn/article-8782-1.html">技术|Linux 包管理基础：apt、yum、dnf 和 pkg</a></p><p><a href="https://www.digitalocean.com/community/tutorials/ubuntu-and-debian-package-management-essentials">Ubuntu 和 Debian 软件包管理要点 |数字海洋 (digitalocean.com)</a></p><p>在了解机制后很容易理解为何需要更新包列表（update &amp;&amp; upgrade），但几乎看到的每个教程在操作前都会进行这个操作，到底多久更新软件包呢？</p><p>国内镜像站大部分是凌晨进行同步，即基本每隔一天就可以刷一下列表</p><h2 id="文件"><a href="#文件" class="headerlink" title="文件"></a>文件</h2><hr><ul><li>文件类型（普通<code>[-]</code>、目录<code>[d]</code>、块设备<code>[b]</code>、字符设备<code>[c]</code>、套接字<code>[s]</code>、管道<code>[p]</code>、链接<code>[l]</code>）</li><li>权限管理（<code>-rwxrwxrwx</code>）</li><li>用户&amp;组</li></ul><h2 id="写在最后"><a href="#写在最后" class="headerlink" title="写在最后"></a>写在最后</h2><hr><p>理解了大框架，命令的学习就会水到渠成，剩下的就是通过<code>--help</code>和<code>man</code>加上搜索引擎来熟悉操作，逐步扩展使用的内容了</p><p>总的来说，背后的理念和各种技术由来可能才是学习阶段需要深刻了解的，以上也只涉及到一部分内容，更多的则需要在日常使用和交流中提升理解</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;回顾了以前学的内容，纯纯的只学命令而没有真的在linux下整过什么东西，恶补有关内容，真正拥抱这个社区&lt;/p&gt;
&lt;p&gt;最近重新把 win+wsl2 拿起来，发现 nvidia 的 cuda 在 win 下安装后通过某种方式直接集成到wsl中了，vscode也接入了wsl，准</summary>
      
    
    
    
    <category term="技术" scheme="http://example.com/categories/%E6%8A%80%E6%9C%AF/"/>
    
    
    <category term="Linux" scheme="http://example.com/tags/Linux/"/>
    
  </entry>
  
  <entry>
    <title>diary</title>
    <link href="http://example.com/2023/10/24/diary/"/>
    <id>http://example.com/2023/10/24/diary/</id>
    <published>2023-10-24T03:55:05.000Z</published>
    <updated>2024-04-25T07:20:07.721Z</updated>
    
    <content type="html"><![CDATA[<p>个人来说，写博客占用学习时间大概是伪命题，因为不写博客可能真的没怎么学习ORZ</p><p>简单写一个学习和日常的记录</p><p>没更新就说明又摸大鱼去了，基本<strong>每周或每两周记录一次</strong>，当周报！</p><h1 id="2024-4-25"><a href="#2024-4-25" class="headerlink" title="2024&#x2F;4&#x2F;25"></a>2024&#x2F;4&#x2F;25</h1><hr><p>腾讯远程课题基本弄完了</p><p>正好组会要每次的论文总结，以后同步点论文分析</p><h1 id="2024-1-22"><a href="#2024-1-22" class="headerlink" title="2024&#x2F;1&#x2F;22"></a>2024&#x2F;1&#x2F;22</h1><hr><p>期末大摆！直到寒假！一些要做的</p><ul><li>学习一下腾讯的远程课题，ue5一点不会</li><li>调研小论文方向的一些最新内容</li><li>养生了，直接开睡</li></ul><h1 id="2023-12-17"><a href="#2023-12-17" class="headerlink" title="2023&#x2F;12&#x2F;17"></a>2023&#x2F;12&#x2F;17</h1><hr><p>试了用RNN网络做图像分类实验，图像的处理太抽象了调了快一天，最后全靠GPT的帮助才把图像数据成功输入到网络里</p><p>下周估计很忙，很多实验课节课，还要准备知识点</p><h1 id="2023-11-19"><a href="#2023-11-19" class="headerlink" title="2023&#x2F;11&#x2F;19"></a>2023&#x2F;11&#x2F;19</h1><hr><p>期中基本结束，但任务还是比较多，一点点来完成</p><p>这周接触了一下ResNet、GAN、Nerf，基本理解其由来和基本实现原理</p><ul><li>准备看一下 games104，会同步更新到博客，学完可能会去看开源引擎的源码，分析其设计和尝试修改，教程的样例引擎Piccolo终于编译过了，最逆天的一集拿MinGW摁编译MSVC的东西</li><li>C&#x2F;C++ 准备多看看代码，了解新特性、编码的 trick 等</li><li>进一步了解 AI 相关，未来有点想研究强化学习和Nerf</li></ul><h1 id="2023-11-11"><a href="#2023-11-11" class="headerlink" title="2023&#x2F;11&#x2F;11"></a>2023&#x2F;11&#x2F;11</h1><hr><p>这周基本补完<code>git</code>、<code>cmake</code>、<code>docker</code>基础，下一步就是提高coding能力</p><p>同时ai也开始看看，继续巩固研究CNN</p><p>期中！算法导论期中考要寄啦，看不懂一点</p><h1 id="2023-11-4"><a href="#2023-11-4" class="headerlink" title="2023&#x2F;11&#x2F;4"></a>2023&#x2F;11&#x2F;4</h1><hr><p>期中阶段，比较忙，赶课堂内容</p><p>wsl用着不错，linux 熟悉了很多</p><p>发现<strong>代码能力太弱</strong>（算法写不出来，开发也两眼抓瞎），准备通过阅读代码和做点项目补上来，还是准备往C++方向走</p><p>英语也不行，练听力，听CSGO比赛的纯净流！</p><h1 id="2023-10-24"><a href="#2023-10-24" class="headerlink" title="2023&#x2F;10&#x2F;24"></a>2023&#x2F;10&#x2F;24</h1><hr><p>这几周摆了很久，复盘了一下自己学过的内容，菜狗了，准备恶补一下</p><p>准备学的一些东西</p><ul><li>接触常用机器学习算法</li><li>docker</li><li>加强 Linux 的使用</li><li>设计模式</li><li>有时间研究一下 Godot 源码</li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;个人来说，写博客占用学习时间大概是伪命题，因为不写博客可能真的没怎么学习ORZ&lt;/p&gt;
&lt;p&gt;简单写一个学习和日常的记录&lt;/p&gt;
&lt;p&gt;没更新就说明又摸大鱼去了，基本&lt;strong&gt;每周或每两周记录一次&lt;/strong&gt;，当周报！&lt;/p&gt;
&lt;h1 id=&quot;2024-4-25</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title>MNIST-about convolution</title>
    <link href="http://example.com/2023/10/08/MNIST-convolution/"/>
    <id>http://example.com/2023/10/08/MNIST-convolution/</id>
    <published>2023-10-08T12:33:15.000Z</published>
    <updated>2023-10-09T07:29:55.757Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Convolution-Neural-Network"><a href="#Convolution-Neural-Network" class="headerlink" title="Convolution Neural Network"></a>Convolution Neural Network</h1><p><a href="https://www.kaggle.com/competitions/digit-recognizer">Digit Recognizer | Kaggle</a></p><p>最近基本补完了卷积的相关知识，拿 MNIST 来实践巩固一下，torch封装的确实好，定义完网络基本就是调参，对外屏蔽了太多的细节，但对于开发者来说就可以把精力花在对具体问题的分析和解决上（努力达到这个目标！）</p><p>准备以这个实验为例子，仔细记录和分析一下CNN重点知识&amp;代码相关的内容</p><p>在参考的时候还看到有纯数学实现的CNN网络，太逆天了</p><hr><p><strong>同样使用 pytorch 构建 CNN，score 为 0.99035</strong></p><p>可以通过数据增强等方法进一步提升模型精度</p><hr><h2 id="Notebook"><a href="#Notebook" class="headerlink" title="Notebook"></a>Notebook</h2><p>基本的一些库，包括<code>torch, numpy, pandas, matplotlib, sklearn</code>，都是必须的库</p><p>有用到<code>TensorDataset, DataLoader</code>，进一步标准化训练流程，也是为了防止显存溢出（最后预测的代码段如果不分batch，会报显存 out of memory）</p><p>最后，<code>device</code>判断 GPU 是否可用，使用 Kaggle 平台的 GPU P100，有卷积层的情况下，CPU 和 GPU 的训练时间差很大</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># This Python 3 environment comes with many helpful analytics libraries installed</span></span><br><span class="line"><span class="comment"># It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python</span></span><br><span class="line"><span class="comment"># For example, here&#x27;s several helpful packages to load</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np <span class="comment"># linear algebra</span></span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd <span class="comment"># data processing, CSV file I/O (e.g. pd.read_csv)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Input data files are available in the read-only &quot;../input/&quot; directory</span></span><br><span class="line"><span class="comment"># For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">for</span> dirname, _, filenames <span class="keyword">in</span> os.walk(<span class="string">&#x27;/kaggle/input&#x27;</span>):</span><br><span class="line">    <span class="keyword">for</span> filename <span class="keyword">in</span> filenames:</span><br><span class="line">        <span class="built_in">print</span>(os.path.join(dirname, filename))</span><br><span class="line"></span><br><span class="line"><span class="comment"># You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using &quot;Save &amp; Run All&quot; </span></span><br><span class="line"><span class="comment"># You can also write temporary files to /kaggle/temp/, but they won&#x27;t be saved outside of the current session</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> TensorDataset</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> torchinfo</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> warnings</span><br><span class="line">warnings.filterwarnings(<span class="string">&quot;ignore&quot;</span>)</span><br><span class="line"></span><br><span class="line">device = torch.device(<span class="string">&#x27;cuda:0&#x27;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&#x27;cpu&#x27;</span>)</span><br><span class="line">device</span><br></pre></td></tr></table></figure><p>device(type&#x3D;’cuda’, index&#x3D;0)</p><hr><h3 id="data-loading"><a href="#data-loading" class="headerlink" title="data loading"></a>data loading</h3><p><code>pd.read_csv</code>导入训练和测试数据</p><p>训练集：42000张图片，每组785个值<code>1 + 1 * 28 * 28 = 785</code>，一个标签&amp;单通道，宽高28pixel的图片</p><p>测试集：28000张图片，每组784个值，单通道，宽高28pixel的图片</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">train_data = pd.read_csv(<span class="string">&#x27;/kaggle/input/digit-recognizer/train.csv&#x27;</span>)</span><br><span class="line">test_data  = pd.read_csv(<span class="string">&#x27;/kaggle/input/digit-recognizer/test.csv&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;train_data.shape:&#x27;</span>, train_data.shape)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;test_data.shape:&#x27;</span>, test_data.shape)</span><br></pre></td></tr></table></figure><p>train_data.shape: (42000, 785)<br>test_data.shape: (28000, 784)</p><hr><h3 id="data-processing"><a href="#data-processing" class="headerlink" title="data processing"></a>data processing</h3><p>分离输入数据和标签</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">X = train_data.iloc[:, train_data.columns != <span class="string">&#x27;label&#x27;</span>]</span><br><span class="line">y = train_data.label.values</span><br><span class="line">X</span><br></pre></td></tr></table></figure><table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th></th>      <th>pixel0</th>      <th>pixel1</th>      <th>pixel2</th>      <th>pixel3</th>      <th>pixel4</th>      <th>pixel5</th>      <th>pixel6</th>      <th>pixel7</th>      <th>pixel8</th>      <th>pixel9</th>      <th>...</th>      <th>pixel774</th>      <th>pixel775</th>      <th>pixel776</th>      <th>pixel777</th>      <th>pixel778</th>      <th>pixel779</th>      <th>pixel780</th>      <th>pixel781</th>      <th>pixel782</th>      <th>pixel783</th>    </tr>  </thead>  <tbody>    <tr>      <th>0</th>      <td>0</td>      <td>0</td>      <td>0</td>      <td>0</td>      <td>0</td>      <td>0</td>      <td>0</td>      <td>0</td>      <td>0</td>      <td>0</td>      <td>...</td>      <td>0</td>      <td>0</td>      <td>0</td>      <td>0</td>      <td>0</td>      <td>0</td>      <td>0</td>      <td>0</td>      <td>0</td>      <td>0</td>    </tr>    <tr>      <th>1</th>      <td>0</td>      <td>0</td>      <td>0</td>      <td>0</td>      <td>0</td>      <td>0</td>      <td>0</td>      <td>0</td>      <td>0</td>      <td>0</td>      <td>...</td>      <td>0</td>      <td>0</td>      <td>0</td>      <td>0</td>      <td>0</td>      <td>0</td>      <td>0</td>      <td>0</td>      <td>0</td>      <td>0</td>    </tr>    <tr>      <th>2</th>      <td>0</td>      <td>0</td>      <td>0</td>      <td>0</td>      <td>0</td>      <td>0</td>      <td>0</td>      <td>0</td>      <td>0</td>      <td>0</td>      <td>...</td>      <td>0</td>      <td>0</td>      <td>0</td>      <td>0</td>      <td>0</td>      <td>0</td>      <td>0</td>      <td>0</td>      <td>0</td>      <td>0</td>    </tr>    <tr>      <th>3</th>      <td>0</td>      <td>0</td>      <td>0</td>      <td>0</td>      <td>0</td>      <td>0</td>      <td>0</td>      <td>0</td>      <td>0</td>      <td>0</td>      <td>...</td>      <td>0</td>      <td>0</td>      <td>0</td>      <td>0</td>      <td>0</td>      <td>0</td>      <td>0</td>      <td>0</td>      <td>0</td>      <td>0</td>    </tr>    <tr>      <th>4</th>      <td>0</td>      <td>0</td>      <td>0</td>      <td>0</td>      <td>0</td>      <td>0</td>      <td>0</td>      <td>0</td>      <td>0</td>      <td>0</td>      <td>...</td>      <td>0</td>      <td>0</td>      <td>0</td>      <td>0</td>      <td>0</td>      <td>0</td>      <td>0</td>      <td>0</td>      <td>0</td>      <td>0</td>    </tr>    <tr>      <th>...</th>      <td>...</td>      <td>...</td>      <td>...</td>      <td>...</td>      <td>...</td>      <td>...</td>      <td>...</td>      <td>...</td>      <td>...</td>      <td>...</td>      <td>...</td>      <td>...</td>      <td>...</td>      <td>...</td>      <td>...</td>      <td>...</td>      <td>...</td>      <td>...</td>      <td>...</td>      <td>...</td>      <td>...</td>    </tr>    <tr>      <th>41995</th>      <td>0</td>      <td>0</td>      <td>0</td>      <td>0</td>      <td>0</td>      <td>0</td>      <td>0</td>      <td>0</td>      <td>0</td>      <td>0</td>      <td>...</td>      <td>0</td>      <td>0</td>      <td>0</td>      <td>0</td>      <td>0</td>      <td>0</td>      <td>0</td>      <td>0</td>      <td>0</td>      <td>0</td>    </tr>    <tr>      <th>41996</th>      <td>0</td>      <td>0</td>      <td>0</td>      <td>0</td>      <td>0</td>      <td>0</td>      <td>0</td>      <td>0</td>      <td>0</td>      <td>0</td>      <td>...</td>      <td>0</td>      <td>0</td>      <td>0</td>      <td>0</td>      <td>0</td>      <td>0</td>      <td>0</td>      <td>0</td>      <td>0</td>      <td>0</td>    </tr>    <tr>      <th>41997</th>      <td>0</td>      <td>0</td>      <td>0</td>      <td>0</td>      <td>0</td>      <td>0</td>      <td>0</td>      <td>0</td>      <td>0</td>      <td>0</td>      <td>...</td>      <td>0</td>      <td>0</td>      <td>0</td>      <td>0</td>      <td>0</td>      <td>0</td>      <td>0</td>      <td>0</td>      <td>0</td>      <td>0</td>    </tr>    <tr>      <th>41998</th>      <td>0</td>      <td>0</td>      <td>0</td>      <td>0</td>      <td>0</td>      <td>0</td>      <td>0</td>      <td>0</td>      <td>0</td>      <td>0</td>      <td>...</td>      <td>0</td>      <td>0</td>      <td>0</td>      <td>0</td>      <td>0</td>      <td>0</td>      <td>0</td>      <td>0</td>      <td>0</td>      <td>0</td>    </tr>    <tr>      <th>41999</th>      <td>0</td>      <td>0</td>      <td>0</td>      <td>0</td>      <td>0</td>      <td>0</td>      <td>0</td>      <td>0</td>      <td>0</td>      <td>0</td>      <td>...</td>      <td>0</td>      <td>0</td>      <td>0</td>      <td>0</td>      <td>0</td>      <td>0</td>      <td>0</td>      <td>0</td>      <td>0</td>      <td>0</td>    </tr>  </tbody></table><p>42000 rows × 784 columns</p><hr><p>输出图片样例，可以观察一下图片（此处就不贴图片了）</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">plt.figure(figsize=(<span class="number">3</span>, <span class="number">3</span>))</span><br><span class="line">plt.title(y[<span class="number">0</span>])</span><br><span class="line">img = X.values[<span class="number">0</span>].reshape(<span class="number">28</span>,<span class="number">28</span>)</span><br><span class="line">plt.axis(<span class="string">&#x27;off&#x27;</span>)</span><br><span class="line">plt.imshow(img, cmap=<span class="string">&#x27;gray&#x27;</span>)</span><br></pre></td></tr></table></figure><hr><p>在训练数据上划分出验证集，以便在训练中测试准确度acc</p><p><code>train_test_split</code>，<code>0.2</code>表示比率</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=<span class="number">0.2</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 0-255 -&gt; 0-1</span></span><br><span class="line">X_train = X_train.values / <span class="number">255</span></span><br><span class="line">X_valid = X_valid.values / <span class="number">255</span></span><br><span class="line">X_train</span><br></pre></td></tr></table></figure><p>array([[0., 0., 0., …, 0., 0., 0.],<br>           [0., 0., 0., …, 0., 0., 0.],<br>           [0., 0., 0., …, 0., 0., 0.],<br>           …,<br>           [0., 0., 0., …, 0., 0., 0.],<br>           [0., 0., 0., …, 0., 0., 0.],<br>           [0., 0., 0., …, 0., 0., 0.]])</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># numpy -&gt; tensor</span></span><br><span class="line">X_train = torch.FloatTensor(X_train)</span><br><span class="line">y_train = torch.LongTensor(y_train)</span><br><span class="line"></span><br><span class="line">X_valid = torch.FloatTensor(X_valid)</span><br><span class="line">y_valid = torch.LongTensor(y_valid)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;X_train.dtype:&#x27;</span>, X_train.dtype)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;y_train.dtype:&#x27;</span>, y_train.dtype)</span><br></pre></td></tr></table></figure><p>X_train.dtype: torch.float32<br>y_train.dtype: torch.int64</p><hr><p>构建dataset 和 dataloader，更好的配合batch</p><p><code>shuffle表示是否打乱数据</code></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">train_ds = TensorDataset(X_train, y_train)</span><br><span class="line">valid_ds = TensorDataset(X_valid, y_valid)</span><br><span class="line"></span><br><span class="line">train_loader = DataLoader(train_ds, batch_size=<span class="number">100</span>, shuffle=<span class="literal">True</span>)</span><br><span class="line">valid_loader = DataLoader(valid_ds, batch_size=<span class="number">100</span>, shuffle=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure><hr><h3 id="Modeling"><a href="#Modeling" class="headerlink" title="Modeling"></a>Modeling</h3><p>图像并不如泰坦尼克幸存者预测有一张特征表，图像只提供了像素</p><p>对于图像分类来说</p><p><strong>卷积层</strong>的作用就是<strong>在保持数据空间结构的基础上进行自动的特征提取</strong>，逐渐抽象出概念</p><p><strong>全连接层</strong>，可以理解为利用卷积层提取出的特征进行分类（相当于逻辑回归）</p><p>由卷积层和线性层的特性，中间需要一个Flatten操作，由多维展平成线性层可接收的一维，但这个参数是需要手动填入的，可以直接推导，也可以在定义模型时利用<code>torchinfo.summary()</code>获取</p><hr><p><strong>通道</strong></p><p>以28*28的RGB图片为例，每张图的shape 就是<code>(3, 28, 28)</code></p><p><strong>每个通道都有单独的卷积核</strong>（如RGB就要配合3个卷积核，一组），通过一次默认的5*5卷积核后RGB三个通道会加到一起 shape<code>(1, 24, 24)</code></p><p>而这样的卷积核有好几组，所以会发现输出通道变多，如若有64组卷积核，则<code>(3, 28, 28) -&gt; (64, 24, 24)</code>，对卷积核或输出的中间图片进行分析，可以认为每组卷积核都在尝试提取图片不同的特征</p><hr><p><code>maxpool</code>：池化，取最大值，也是一个非线性操作（比取均值更好），主要用以快速降维</p><p><code>dropout</code>：以概率p丢弃计算图中的一些结点，防止由于学习到数据中的噪声而导致的过拟合（一个很好的解决过拟合的方案）</p><hr><p>定义如下（5的卷积核比3的有更高的概括性）：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Digit_Recognizer_Model</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Architecture summary:</span></span><br><span class="line"><span class="string">      - Layer 1: Convolution 1 &gt; Activation (ReLU) &gt; Pooling 1 &gt; Dropout 1</span></span><br><span class="line"><span class="string">      - Layer 2: Convolution 2 &gt; Activation (ReLU) &gt; Pooling 2 &gt; Dropout 2 &gt; Flatten</span></span><br><span class="line"><span class="string">      - Layer 3: Linear 1 &gt; Activation (ReLU) &gt; Dropout 3</span></span><br><span class="line"><span class="string">      - Layer 4: Linear 2 &gt; Activation (ReLU) &gt; Dropout 4</span></span><br><span class="line"><span class="string">      - Layer 5: Output &gt; Activation (Softmax)</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># layer 1</span></span><br><span class="line">        self.conv1 = nn.Conv2d(in_channels=<span class="number">1</span>, out_channels=<span class="number">64</span>, kernel_size=<span class="number">5</span>, stride=<span class="number">1</span>, padding=<span class="number">0</span>)</span><br><span class="line">        self.pool1 = nn.MaxPool2d(kernel_size=(<span class="number">2</span>, <span class="number">2</span>))</span><br><span class="line">        self.drop1 = nn.Dropout(p=<span class="number">0.3</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="comment">#layer 2</span></span><br><span class="line">        self.conv2 = nn.Conv2d(in_channels=<span class="number">64</span>, out_channels=<span class="number">128</span>, kernel_size=<span class="number">5</span>, stride=<span class="number">1</span>, padding=<span class="number">0</span>)</span><br><span class="line">        self.pool2 = nn.MaxPool2d(kernel_size=(<span class="number">2</span>, <span class="number">2</span>))</span><br><span class="line">        self.drop2 = nn.Dropout(p=<span class="number">0.4</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="comment">#layer 3</span></span><br><span class="line">        self.fc1 = nn.Linear(in_features=<span class="number">128</span> * <span class="number">4</span> * <span class="number">4</span>, out_features=<span class="number">64</span>)</span><br><span class="line">        self.drop3 = nn.Dropout(p=<span class="number">0.4</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="comment">#layer 4</span></span><br><span class="line">        self.fc2 = nn.Linear(in_features=<span class="number">64</span>, out_features=<span class="number">32</span>)</span><br><span class="line">        self.drop4 = nn.Dropout(p=<span class="number">0.4</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="comment">#layer 5</span></span><br><span class="line">        self.fc3 = nn.Linear(in_features=<span class="number">32</span>, out_features=<span class="number">10</span>)</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = self.drop1(self.pool1(F.relu(self.conv1(x)))) <span class="comment"># layer 1</span></span><br><span class="line">        x = self.drop2(self.pool2(F.relu(self.conv2(x)))) <span class="comment"># layer 2</span></span><br><span class="line">        x = x.view(-<span class="number">1</span>, <span class="number">128</span> * <span class="number">4</span> * <span class="number">4</span>) <span class="comment"># flatten</span></span><br><span class="line">        x = self.drop3(F.relu(self.fc1(x)))</span><br><span class="line">        x = self.drop4(F.relu(self.fc2(x)))</span><br><span class="line">        x = self.fc3(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">model = Digit_Recognizer_Model().to(device)</span><br><span class="line">criterion = nn.CrossEntropyLoss()</span><br><span class="line">optimizer = optim.Adam(model.parameters(), lr=<span class="number">0.001</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torchinfo.summary(model, (<span class="number">1</span>,<span class="number">28</span>,<span class="number">28</span>), col_names=(<span class="string">&#x27;input_size&#x27;</span>, <span class="string">&#x27;output_size&#x27;</span>, <span class="string">&#x27;num_params&#x27;</span>, <span class="string">&#x27;kernel_size&#x27;</span>))</span><br></pre></td></tr></table></figure><pre><code>============================================================================================================================================Layer (type:depth-idx)                   Input Shape               Output Shape              Param #                   Kernel Shape============================================================================================================================================Digit_Recognizer_Model                   [1, 28, 28]               [1, 10]                   --                        --├─Conv2d: 1-1                            [1, 28, 28]               [64, 24, 24]              1,664                     [5, 5]├─MaxPool2d: 1-2                         [64, 24, 24]              [64, 12, 12]              --                        [2, 2]├─Dropout: 1-3                           [64, 12, 12]              [64, 12, 12]              --                        --├─Conv2d: 1-4                            [64, 12, 12]              [128, 8, 8]               204,928                   [5, 5]├─MaxPool2d: 1-5                         [128, 8, 8]               [128, 4, 4]               --                        [2, 2]├─Dropout: 1-6                           [128, 4, 4]               [128, 4, 4]               --                        --├─Linear: 1-7                            [1, 2048]                 [1, 64]                   131,136                   --├─Dropout: 1-8                           [1, 64]                   [1, 64]                   --                        --├─Linear: 1-9                            [1, 64]                   [1, 32]                   2,080                     --├─Dropout: 1-10                          [1, 32]                   [1, 32]                   --                        --├─Linear: 1-11                           [1, 32]                   [1, 10]                   330                       --============================================================================================================================================Total params: 340,138Trainable params: 340,138Non-trainable params: 0Total mult-adds (M): 212.54============================================================================================================================================Input size (MB): 0.00Forward/backward pass size (MB): 0.36Params size (MB): 1.36Estimated Total Size (MB): 1.73============================================================================================================================================</code></pre><p>以下给出shape变化的推导：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">(1,28,28)  --- 5*5 conv-kernal ---&gt; (64,24,24) # 24 = 28-5+1</span><br><span class="line">(64,24,24) ---    2*2 maxpool  ---&gt; (64,12,12) </span><br><span class="line">(64,12,12) --- 5*5 conv-kernal ---&gt; (128,8,8)  # 8 = 12-5+1</span><br><span class="line">(128,8,8)  ---    2*2 maxpool  ---&gt; (128,4,4)</span><br></pre></td></tr></table></figure><hr><h3 id="Train"><a href="#Train" class="headerlink" title="Train"></a>Train</h3><p>每个epoch都利用验证集测试准确率，感觉最好分别封装成函数</p><p>特别注意<code>model.train(), model.eval()</code>，<strong>在eval状态下模型会固定dropout的参数采用训练值</strong></p><p>说一下<code>argmax(1)</code>输出每行中最大值的索引，模型<strong>最后的全连接层输出包含10个概率值的向量</strong>，通过softmax转换成总和为1的一组数据，每行的最大值即是最可能的数字，<code>argmax()</code><strong>可以视作softmax的函数版本</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># --------- train ----------</span></span><br><span class="line">epochs = <span class="number">15</span></span><br><span class="line">size = <span class="built_in">len</span>(train_loader.dataset)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(epochs):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;epoch:<span class="subst">&#123;epoch+<span class="number">1</span>&#125;</span>\n---------------------------&#x27;</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># ------ train -------</span></span><br><span class="line">    <span class="keyword">for</span> batch, (X, y) <span class="keyword">in</span> <span class="built_in">enumerate</span>(train_loader):</span><br><span class="line">        model.train()</span><br><span class="line">        X = X.view(-<span class="number">1</span>, <span class="number">1</span>, <span class="number">28</span>, <span class="number">28</span>).to(device)</span><br><span class="line">        y = y.to(device)</span><br><span class="line">        <span class="comment"># forward</span></span><br><span class="line">        output = model(X)</span><br><span class="line">        train_loss = criterion(output, y)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># backward</span></span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        train_loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> batch % <span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">f&#x27;loss: <span class="subst">&#123;train_loss.item():&gt;6f&#125;</span> [<span class="subst">&#123;(batch+<span class="number">1</span>)*<span class="built_in">len</span>(X):&gt;5d&#125;</span>/<span class="subst">&#123;size&#125;</span>]&#x27;</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># ------ test -------</span></span><br><span class="line">    model.<span class="built_in">eval</span>()</span><br><span class="line">    valid_loss, acc = <span class="number">0</span>, <span class="number">0</span></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        <span class="keyword">for</span> X, y <span class="keyword">in</span> valid_loader:</span><br><span class="line">            X = X.view(-<span class="number">1</span>, <span class="number">1</span>, <span class="number">28</span>, <span class="number">28</span>).to(device)</span><br><span class="line">            y = y.to(device)</span><br><span class="line">            </span><br><span class="line">            pred = model(X)</span><br><span class="line">            valid_loss += criterion(pred, y).item()</span><br><span class="line">            acc += (pred.argmax(<span class="number">1</span>) == y).<span class="built_in">type</span>(torch.<span class="built_in">float</span>).<span class="built_in">sum</span>().item()</span><br><span class="line">            </span><br><span class="line">        valid_loss /= <span class="built_in">len</span>(valid_loader.dataset)/<span class="number">100</span></span><br><span class="line">        acc /= <span class="built_in">len</span>(valid_loader.dataset)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&#x27;acc: <span class="subst">&#123;acc*<span class="number">100</span>:&gt;<span class="number">0.2</span>f&#125;</span>%, average_loss: <span class="subst">&#123;valid_loss:&gt;8f&#125;</span>&#x27;</span>)</span><br></pre></td></tr></table></figure><pre><code>epoch:1---------------------------loss: 2.310378 [  100/33600]loss: 0.830918 [10100/33600]loss: 0.440694 [20100/33600]loss: 0.341579 [30100/33600]acc: 96.49%, average_loss: 0.135642epoch:2---------------------------loss: 0.354336 [  100/33600]loss: 0.339530 [10100/33600]loss: 0.192688 [20100/33600]loss: 0.228617 [30100/33600]acc: 97.63%, average_loss: 0.083003epoch:3---------------------------loss: 0.310060 [  100/33600]loss: 0.095043 [10100/33600]loss: 0.387724 [20100/33600]loss: 0.102543 [30100/33600]acc: 97.93%, average_loss: 0.068741epoch:4---------------------------loss: 0.141973 [  100/33600]loss: 0.352170 [10100/33600]loss: 0.187784 [20100/33600]loss: 0.144804 [30100/33600]acc: 98.38%, average_loss: 0.056450epoch:5---------------------------loss: 0.122953 [  100/33600]loss: 0.209291 [10100/33600]loss: 0.116453 [20100/33600]loss: 0.143049 [30100/33600]acc: 98.62%, average_loss: 0.048829epoch:6---------------------------loss: 0.142570 [  100/33600]loss: 0.154392 [10100/33600]loss: 0.080456 [20100/33600]loss: 0.071681 [30100/33600]acc: 98.48%, average_loss: 0.054392epoch:7---------------------------loss: 0.049880 [  100/33600]loss: 0.145781 [10100/33600]loss: 0.151001 [20100/33600]loss: 0.096750 [30100/33600]acc: 98.67%, average_loss: 0.043555epoch:8---------------------------loss: 0.047690 [  100/33600]loss: 0.049740 [10100/33600]loss: 0.089974 [20100/33600]loss: 0.074760 [30100/33600]acc: 98.92%, average_loss: 0.039260epoch:9---------------------------loss: 0.051311 [  100/33600]loss: 0.116427 [10100/33600]loss: 0.159870 [20100/33600]loss: 0.162403 [30100/33600]acc: 98.89%, average_loss: 0.042807epoch:10---------------------------loss: 0.085799 [  100/33600]loss: 0.131933 [10100/33600]loss: 0.095897 [20100/33600]loss: 0.053006 [30100/33600]acc: 98.93%, average_loss: 0.042807epoch:11---------------------------loss: 0.034143 [  100/33600]loss: 0.031655 [10100/33600]loss: 0.090933 [20100/33600]loss: 0.075254 [30100/33600]acc: 98.86%, average_loss: 0.042070epoch:12---------------------------loss: 0.071062 [  100/33600]loss: 0.058759 [10100/33600]loss: 0.090834 [20100/33600]loss: 0.025837 [30100/33600]acc: 99.01%, average_loss: 0.041503epoch:13---------------------------loss: 0.069484 [  100/33600]loss: 0.068897 [10100/33600]loss: 0.033803 [20100/33600]loss: 0.023075 [30100/33600]acc: 99.14%, average_loss: 0.039555epoch:14---------------------------loss: 0.024676 [  100/33600]loss: 0.042386 [10100/33600]loss: 0.051634 [20100/33600]loss: 0.084135 [30100/33600]acc: 98.98%, average_loss: 0.040245epoch:15---------------------------loss: 0.099463 [  100/33600]loss: 0.039321 [10100/33600]loss: 0.058406 [20100/33600]loss: 0.113745 [30100/33600]acc: 99.30%, average_loss: 0.034384</code></pre><p>用GPU炼真的很快，CPU只能拿来应付线性层</p><hr><h3 id="Submission"><a href="#Submission" class="headerlink" title="Submission"></a>Submission</h3><p>用pandas构建一下提交数据</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">submission_rows = [[<span class="string">&#x27;ImageId&#x27;</span>, <span class="string">&#x27;Label&#x27;</span>]]</span><br><span class="line">X_test = test_data.values / <span class="number">255</span></span><br><span class="line">X_test = torch.FloatTensor(X_test)</span><br><span class="line">y_test = np.zeros(X_test.shape)</span><br><span class="line">y_test = torch.LongTensor(y_test)</span><br><span class="line">test_ds = TensorDataset(X_test, y_test)</span><br><span class="line">test_loader = DataLoader(test_ds, batch_size=<span class="number">100</span>, shuffle=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    image_id = <span class="number">1</span></span><br><span class="line">    <span class="keyword">for</span> X, _ <span class="keyword">in</span> test_loader:</span><br><span class="line">        model.<span class="built_in">eval</span>()</span><br><span class="line">        X = X.view(-<span class="number">1</span>, <span class="number">1</span>, <span class="number">28</span>, <span class="number">28</span>).to(device)</span><br><span class="line">        pred = model(X).argmax(<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> pred:</span><br><span class="line">            submission_rows.append([image_id, i.item()])</span><br><span class="line">            image_id += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">submission = pd.DataFrame(submission_rows)</span><br><span class="line">submission.columns = submission.iloc[<span class="number">0</span>]</span><br><span class="line">submission = submission.drop(<span class="number">0</span>, axis=<span class="number">0</span>)</span><br><span class="line">submission.to_csv(<span class="string">&#x27;submission.csv&#x27;</span>, index=<span class="literal">False</span>)</span><br><span class="line">submission</span><br></pre></td></tr></table></figure><table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th></th>      <th>ImageId</th>      <th>Label</th>    </tr>  </thead>  <tbody>    <tr>      <th>1</th>      <td>1</td>      <td>2</td>    </tr>    <tr>      <th>2</th>      <td>2</td>      <td>0</td>    </tr>    <tr>      <th>3</th>      <td>3</td>      <td>9</td>    </tr>    <tr>      <th>4</th>      <td>4</td>      <td>9</td>    </tr>    <tr>      <th>5</th>      <td>5</td>      <td>3</td>    </tr>    <tr>      <th>...</th>      <td>...</td>      <td>...</td>    </tr>    <tr>      <th>27996</th>      <td>27996</td>      <td>9</td>    </tr>    <tr>      <th>27997</th>      <td>27997</td>      <td>7</td>    </tr>    <tr>      <th>27998</th>      <td>27998</td>      <td>3</td>    </tr>    <tr>      <th>27999</th>      <td>27999</td>      <td>9</td>    </tr>    <tr>      <th>28000</th>      <td>28000</td>      <td>2</td>    </tr>  </tbody></table><p>28000 rows × 2 columns</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;Convolution-Neural-Network&quot;&gt;&lt;a href=&quot;#Convolution-Neural-Network&quot; class=&quot;headerlink&quot; title=&quot;Convolution Neural Network&quot;&gt;&lt;/a&gt;Convolut</summary>
      
    
    
    
    <category term="机器学习" scheme="http://example.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="pytorch" scheme="http://example.com/tags/pytorch/"/>
    
    <category term="CNN" scheme="http://example.com/tags/CNN/"/>
    
    <category term="ML" scheme="http://example.com/tags/ML/"/>
    
  </entry>
  
  <entry>
    <title>Titanic - start from disaster</title>
    <link href="http://example.com/2023/10/06/titanic/"/>
    <id>http://example.com/2023/10/06/titanic/</id>
    <published>2023-10-06T12:32:59.000Z</published>
    <updated>2023-10-06T15:03:33.829Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Titanic-PyTorch"><a href="#Titanic-PyTorch" class="headerlink" title="Titanic_PyTorch"></a>Titanic_PyTorch</h1><p><a href="https://www.kaggle.com/competitions/titanic">Titanic - Machine Learning from Disaster | Kaggle</a></p><p>目标是对泰坦尼克号的幸存者做预测，一个很好的试验田</p><p>使用 pytorch 构建的 DNN 进行训练和预测，准确率约 0.772</p><p>通过查看相关文章，实验数据存在一定的特殊性，高准确率基本都是先通过特征分析选取强特征再使用决策树（随机森林）的方式进行判断</p><hr><p>总结了一下大致的过程：</p><ol><li>载入数据</li><li>分析数据</li><li>预处理数据（空值，独热编码，标签，标准化，类型转换）</li><li>网络模型定义（网络，损失函数，优化器，超参数）</li><li>训练（mini-batch SGD）</li><li>预测</li></ol><p>在 CPU 和 GPU 上都实现了一下，但是数据量小看不太出来差异，同时也需要对测试数据进一步划分以求出最佳epoch</p><p>其实还可以进一步加入 Kfold 操作，对各种超参数也没有怎么理解</p><hr><h2 id="Notebook"><a href="#Notebook" class="headerlink" title="Notebook"></a>Notebook</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np <span class="comment"># linear algebra</span></span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd <span class="comment"># data processing, CSV file I/O (e.g. pd.read_csv)</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">for</span> dirname, _, filenames <span class="keyword">in</span> os.walk(<span class="string">&#x27;/kaggle/input&#x27;</span>):</span><br><span class="line">    <span class="keyword">for</span> filename <span class="keyword">in</span> filenames:</span><br><span class="line">        <span class="built_in">print</span>(os.path.join(dirname, filename))</span><br></pre></td></tr></table></figure><p>&#x2F;kaggle&#x2F;input&#x2F;titanic&#x2F;train.csv<br>&#x2F;kaggle&#x2F;input&#x2F;titanic&#x2F;test.csv<br>&#x2F;kaggle&#x2F;input&#x2F;titanic&#x2F;gender_submission.csv</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> tqdm <span class="keyword">import</span> tqdm  <span class="comment"># progress bar</span></span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler</span><br><span class="line"></span><br><span class="line"><span class="comment"># ------------- pytorch ---------------</span></span><br><span class="line"><span class="keyword">import</span> torch                    </span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F <span class="comment"># all functions including loss and activation function</span></span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim     <span class="comment"># optimization algorithm</span></span><br><span class="line"></span><br><span class="line">device = torch.device(<span class="string">&#x27;cuda:0&#x27;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&#x27;cpu&#x27;</span>) <span class="comment"># GPU or CPU</span></span><br><span class="line">device</span><br></pre></td></tr></table></figure><p>device(type&#x3D;’cpu’)</p><h3 id="load-data"><a href="#load-data" class="headerlink" title="load data"></a>load data</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -------------- load data ------------------</span></span><br><span class="line"></span><br><span class="line">train_data = pd.read_csv(<span class="string">&#x27;/kaggle/input/titanic/train.csv&#x27;</span>)</span><br><span class="line">test_data  = pd.read_csv(<span class="string">&#x27;/kaggle/input/titanic/test.csv&#x27;</span>)</span><br><span class="line">sub = pd.read_csv(<span class="string">&#x27;/kaggle/input/titanic/gender_submission.csv&#x27;</span>)</span><br><span class="line">train_data.head(<span class="number">3</span>)</span><br></pre></td></tr></table></figure><table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th></th>      <th>PassengerId</th>      <th>Survived</th>      <th>Pclass</th>      <th>Name</th>      <th>Sex</th>      <th>Age</th>      <th>SibSp</th>      <th>Parch</th>      <th>Ticket</th>      <th>Fare</th>      <th>Cabin</th>      <th>Embarked</th>    </tr>  </thead>  <tbody>    <tr>      <th>0</th>      <td>1</td>      <td>0</td>      <td>3</td>      <td>Braund, Mr. Owen Harris</td>      <td>male</td>      <td>22.0</td>      <td>1</td>      <td>0</td>      <td>A/5 21171</td>      <td>7.2500</td>      <td>NaN</td>      <td>S</td>    </tr>    <tr>      <th>1</th>      <td>2</td>      <td>1</td>      <td>1</td>      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>      <td>female</td>      <td>38.0</td>      <td>1</td>      <td>0</td>      <td>PC 17599</td>      <td>71.2833</td>      <td>C85</td>      <td>C</td>    </tr>    <tr>      <th>2</th>      <td>3</td>      <td>1</td>      <td>3</td>      <td>Heikkinen, Miss. Laina</td>      <td>female</td>      <td>26.0</td>      <td>0</td>      <td>0</td>      <td>STON/O2. 3101282</td>      <td>7.9250</td>      <td>NaN</td>      <td>S</td>    </tr>  </tbody></table><hr><h3 id="analyzing-data"><a href="#analyzing-data" class="headerlink" title="analyzing data"></a>analyzing data</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">train_data.info()</span><br></pre></td></tr></table></figure><pre><code>&lt;class &#39;pandas.core.frame.DataFrame&#39;&gt;RangeIndex: 891 entries, 0 to 890Data columns (total 12 columns): #   Column       Non-Null Count  Dtype  ---  ------       --------------  -----   0   PassengerId  891 non-null    int64   1   Survived     891 non-null    int64   2   Pclass       891 non-null    int64   3   Name         891 non-null    object  4   Sex          891 non-null    object  5   Age          714 non-null    float64 6   SibSp        891 non-null    int64   7   Parch        891 non-null    int64   8   Ticket       891 non-null    object  9   Fare         891 non-null    float64 10  Cabin        204 non-null    object  11  Embarked     889 non-null    object dtypes: float64(2), int64(5), object(5)memory usage: 83.7+ KB</code></pre><p>可以看出<code>Age, Cabin, Embarked</code>列存在数据缺失，后续进一步填充或者编码</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">train_data.describe()</span><br></pre></td></tr></table></figure><table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th></th>      <th>PassengerId</th>      <th>Survived</th>      <th>Pclass</th>      <th>Age</th>      <th>SibSp</th>      <th>Parch</th>      <th>Fare</th>    </tr>  </thead>  <tbody>    <tr>      <th>count</th>      <td>891.000000</td>      <td>891.000000</td>      <td>891.000000</td>      <td>714.000000</td>      <td>891.000000</td>      <td>891.000000</td>      <td>891.000000</td>    </tr>    <tr>      <th>mean</th>      <td>446.000000</td>      <td>0.383838</td>      <td>2.308642</td>      <td>29.699118</td>      <td>0.523008</td>      <td>0.381594</td>      <td>32.204208</td>    </tr>    <tr>      <th>std</th>      <td>257.353842</td>      <td>0.486592</td>      <td>0.836071</td>      <td>14.526497</td>      <td>1.102743</td>      <td>0.806057</td>      <td>49.693429</td>    </tr>    <tr>      <th>min</th>      <td>1.000000</td>      <td>0.000000</td>      <td>1.000000</td>      <td>0.420000</td>      <td>0.000000</td>      <td>0.000000</td>      <td>0.000000</td>    </tr>    <tr>      <th>25%</th>      <td>223.500000</td>      <td>0.000000</td>      <td>2.000000</td>      <td>20.125000</td>      <td>0.000000</td>      <td>0.000000</td>      <td>7.910400</td>    </tr>    <tr>      <th>50%</th>      <td>446.000000</td>      <td>0.000000</td>      <td>3.000000</td>      <td>28.000000</td>      <td>0.000000</td>      <td>0.000000</td>      <td>14.454200</td>    </tr>    <tr>      <th>75%</th>      <td>668.500000</td>      <td>1.000000</td>      <td>3.000000</td>      <td>38.000000</td>      <td>1.000000</td>      <td>0.000000</td>      <td>31.000000</td>    </tr>    <tr>      <th>max</th>      <td>891.000000</td>      <td>1.000000</td>      <td>3.000000</td>      <td>80.000000</td>      <td>8.000000</td>      <td>6.000000</td>      <td>512.329200</td>    </tr>  </tbody></table>进一步了解数据分布和一些特征<hr><h3 id="Preprocess-the-data"><a href="#Preprocess-the-data" class="headerlink" title="Preprocess the data"></a>Preprocess the data</h3><p>逐步展示数据的变化流程</p><ul><li>filling the NaN values</li><li>label encoding</li><li>One-hot encoding</li><li>split the data</li><li>define features and target</li><li>Normalization</li><li>numpy to tensor</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Age - filling the NaN values</span></span><br><span class="line">age_mean = train_data[<span class="string">&#x27;Age&#x27;</span>].dropna().mean()</span><br><span class="line">train_data[<span class="string">&#x27;Age&#x27;</span>].fillna(age_mean,inplace=<span class="literal">True</span>)</span><br><span class="line">test_data[<span class="string">&#x27;Age&#x27;</span>].fillna(age_mean,inplace=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Sex - label encoding</span></span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> LabelEncoder</span><br><span class="line">le = LabelEncoder()</span><br><span class="line">le.fit(train_data[<span class="string">&quot;Sex&quot;</span>])</span><br><span class="line">train_data[<span class="string">&quot;Sex&quot;</span>] = le.transform(train_data[<span class="string">&quot;Sex&quot;</span>])</span><br><span class="line">test_data[<span class="string">&quot;Sex&quot;</span>]  = le.transform(test_data[<span class="string">&quot;Sex&quot;</span>])</span><br><span class="line">train_data.head(<span class="number">5</span>)</span><br></pre></td></tr></table></figure><table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th></th>      <th>PassengerId</th>      <th>Survived</th>      <th>Pclass</th>      <th>Name</th>      <th>Sex</th>      <th>Age</th>      <th>SibSp</th>      <th>Parch</th>      <th>Ticket</th>      <th>Fare</th>      <th>Cabin</th>      <th>Embarked</th>    </tr>  </thead>  <tbody>    <tr>      <th>0</th>      <td>1</td>      <td>0</td>      <td>3</td>      <td>Braund, Mr. Owen Harris</td>      <td>1</td>      <td>22.0</td>      <td>1</td>      <td>0</td>      <td>A/5 21171</td>      <td>7.2500</td>      <td>NaN</td>      <td>S</td>    </tr>    <tr>      <th>1</th>      <td>2</td>      <td>1</td>      <td>1</td>      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>      <td>0</td>      <td>38.0</td>      <td>1</td>      <td>0</td>      <td>PC 17599</td>      <td>71.2833</td>      <td>C85</td>      <td>C</td>    </tr>    <tr>      <th>2</th>      <td>3</td>      <td>1</td>      <td>3</td>      <td>Heikkinen, Miss. Laina</td>      <td>0</td>      <td>26.0</td>      <td>0</td>      <td>0</td>      <td>STON/O2. 3101282</td>      <td>7.9250</td>      <td>NaN</td>      <td>S</td>    </tr>    <tr>      <th>3</th>      <td>4</td>      <td>1</td>      <td>1</td>      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>      <td>0</td>      <td>35.0</td>      <td>1</td>      <td>0</td>      <td>113803</td>      <td>53.1000</td>      <td>C123</td>      <td>S</td>    </tr>    <tr>      <th>4</th>      <td>5</td>      <td>0</td>      <td>3</td>      <td>Allen, Mr. William Henry</td>      <td>1</td>      <td>35.0</td>      <td>0</td>      <td>0</td>      <td>373450</td>      <td>8.0500</td>      <td>NaN</td>      <td>S</td>    </tr>  </tbody></table><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Embarked - one hot encoding</span></span><br><span class="line">temp    = pd.concat([train_data,test_data],axis = <span class="number">0</span>)</span><br><span class="line">temp_em = pd.get_dummies(temp[<span class="string">&quot;Embarked&quot;</span>],dummy_na=<span class="literal">True</span>)</span><br><span class="line">temp    = pd.concat([temp, temp_em],axis = <span class="number">1</span>)</span><br><span class="line">temp.head(<span class="number">5</span>)</span><br></pre></td></tr></table></figure><table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th></th>      <th>PassengerId</th>      <th>Survived</th>      <th>Pclass</th>      <th>Name</th>      <th>Sex</th>      <th>Age</th>      <th>SibSp</th>      <th>Parch</th>      <th>Ticket</th>      <th>Fare</th>      <th>Cabin</th>      <th>Embarked</th>      <th>C</th>      <th>Q</th>      <th>S</th>      <th>NaN</th>    </tr>  </thead>  <tbody>    <tr>      <th>0</th>      <td>1</td>      <td>0.0</td>      <td>3</td>      <td>Braund, Mr. Owen Harris</td>      <td>1</td>      <td>22.0</td>      <td>1</td>      <td>0</td>      <td>A/5 21171</td>      <td>7.2500</td>      <td>NaN</td>      <td>S</td>      <td>False</td>      <td>False</td>      <td>True</td>      <td>False</td>    </tr>    <tr>      <th>1</th>      <td>2</td>      <td>1.0</td>      <td>1</td>      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>      <td>0</td>      <td>38.0</td>      <td>1</td>      <td>0</td>      <td>PC 17599</td>      <td>71.2833</td>      <td>C85</td>      <td>C</td>      <td>True</td>      <td>False</td>      <td>False</td>      <td>False</td>    </tr>    <tr>      <th>2</th>      <td>3</td>      <td>1.0</td>      <td>3</td>      <td>Heikkinen, Miss. Laina</td>      <td>0</td>      <td>26.0</td>      <td>0</td>      <td>0</td>      <td>STON/O2. 3101282</td>      <td>7.9250</td>      <td>NaN</td>      <td>S</td>      <td>False</td>      <td>False</td>      <td>True</td>      <td>False</td>    </tr>    <tr>      <th>3</th>      <td>4</td>      <td>1.0</td>      <td>1</td>      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>      <td>0</td>      <td>35.0</td>      <td>1</td>      <td>0</td>      <td>113803</td>      <td>53.1000</td>      <td>C123</td>      <td>S</td>      <td>False</td>      <td>False</td>      <td>True</td>      <td>False</td>    </tr>    <tr>      <th>4</th>      <td>5</td>      <td>0.0</td>      <td>3</td>      <td>Allen, Mr. William Henry</td>      <td>1</td>      <td>35.0</td>      <td>0</td>      <td>0</td>      <td>373450</td>      <td>8.0500</td>      <td>NaN</td>      <td>S</td>      <td>False</td>      <td>False</td>      <td>True</td>      <td>False</td>    </tr>  </tbody></table><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># split the data</span></span><br><span class="line">train = temp.iloc[:<span class="built_in">len</span>(train_data),:]</span><br><span class="line">test  = temp.iloc[<span class="built_in">len</span>(train_data):,:]</span><br><span class="line"></span><br><span class="line"><span class="comment"># define features and target</span></span><br><span class="line">features = [<span class="string">&quot;Pclass&quot;</span>,<span class="string">&quot;Sex&quot;</span>,<span class="string">&quot;Age&quot;</span>,<span class="string">&quot;SibSp&quot;</span>,<span class="string">&quot;Parch&quot;</span>,<span class="string">&quot;C&quot;</span>,<span class="string">&quot;Q&quot;</span>,<span class="string">&quot;S&quot;</span>,np.nan]</span><br><span class="line">target   = <span class="string">&quot;Survived&quot;</span></span><br><span class="line">train[features].head(<span class="number">5</span>)</span><br></pre></td></tr></table></figure><table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th></th>      <th>Pclass</th>      <th>Sex</th>      <th>Age</th>      <th>SibSp</th>      <th>Parch</th>      <th>C</th>      <th>Q</th>      <th>S</th>      <th>NaN</th>    </tr>  </thead>  <tbody>    <tr>      <th>0</th>      <td>3</td>      <td>1</td>      <td>22.0</td>      <td>1</td>      <td>0</td>      <td>False</td>      <td>False</td>      <td>True</td>      <td>False</td>    </tr>    <tr>      <th>1</th>      <td>1</td>      <td>0</td>      <td>38.0</td>      <td>1</td>      <td>0</td>      <td>True</td>      <td>False</td>      <td>False</td>      <td>False</td>    </tr>    <tr>      <th>2</th>      <td>3</td>      <td>0</td>      <td>26.0</td>      <td>0</td>      <td>0</td>      <td>False</td>      <td>False</td>      <td>True</td>      <td>False</td>    </tr>    <tr>      <th>3</th>      <td>1</td>      <td>0</td>      <td>35.0</td>      <td>1</td>      <td>0</td>      <td>False</td>      <td>False</td>      <td>True</td>      <td>False</td>    </tr>    <tr>      <th>4</th>      <td>3</td>      <td>1</td>      <td>35.0</td>      <td>0</td>      <td>0</td>      <td>False</td>      <td>False</td>      <td>True</td>      <td>False</td>    </tr>  </tbody></table><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">train_X  = np.array(train[features])</span><br><span class="line">train_Y  = np.array(train[target])</span><br><span class="line">test_X   = np.array(test[features])</span><br><span class="line">test_Y   = np.array(test[target])</span><br><span class="line"></span><br><span class="line"><span class="comment"># Normalization</span></span><br><span class="line">Scaler  = StandardScaler()</span><br><span class="line">train_X = Scaler.fit_transform(train_X)</span><br><span class="line">test_X  = Scaler.fit_transform(test_X)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">temp = pd.DataFrame(train_X, columns=features)</span><br><span class="line">temp</span><br></pre></td></tr></table></figure><table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th></th>      <th>Pclass</th>      <th>Sex</th>      <th>Age</th>      <th>SibSp</th>      <th>Parch</th>      <th>C</th>      <th>Q</th>      <th>S</th>      <th>NaN</th>    </tr>  </thead>  <tbody>    <tr>      <th>0</th>      <td>0.827377</td>      <td>0.737695</td>      <td>-0.592481</td>      <td>0.432793</td>      <td>-0.473674</td>      <td>-0.482043</td>      <td>-0.307562</td>      <td>0.619306</td>      <td>-0.047431</td>    </tr>    <tr>      <th>1</th>      <td>-1.566107</td>      <td>-1.355574</td>      <td>0.638789</td>      <td>0.432793</td>      <td>-0.473674</td>      <td>2.074505</td>      <td>-0.307562</td>      <td>-1.614710</td>      <td>-0.047431</td>    </tr>    <tr>      <th>2</th>      <td>0.827377</td>      <td>-1.355574</td>      <td>-0.284663</td>      <td>-0.474545</td>      <td>-0.473674</td>      <td>-0.482043</td>      <td>-0.307562</td>      <td>0.619306</td>      <td>-0.047431</td>    </tr>    <tr>      <th>3</th>      <td>-1.566107</td>      <td>-1.355574</td>      <td>0.407926</td>      <td>0.432793</td>      <td>-0.473674</td>      <td>-0.482043</td>      <td>-0.307562</td>      <td>0.619306</td>      <td>-0.047431</td>    </tr>    <tr>      <th>4</th>      <td>0.827377</td>      <td>0.737695</td>      <td>0.407926</td>      <td>-0.474545</td>      <td>-0.473674</td>      <td>-0.482043</td>      <td>-0.307562</td>      <td>0.619306</td>      <td>-0.047431</td>    </tr>    <tr>      <th>...</th>      <td>...</td>      <td>...</td>      <td>...</td>      <td>...</td>      <td>...</td>      <td>...</td>      <td>...</td>      <td>...</td>      <td>...</td>    </tr>    <tr>      <th>886</th>      <td>-0.369365</td>      <td>0.737695</td>      <td>-0.207709</td>      <td>-0.474545</td>      <td>-0.473674</td>      <td>-0.482043</td>      <td>-0.307562</td>      <td>0.619306</td>      <td>-0.047431</td>    </tr>    <tr>      <th>887</th>      <td>-1.566107</td>      <td>-1.355574</td>      <td>-0.823344</td>      <td>-0.474545</td>      <td>-0.473674</td>      <td>-0.482043</td>      <td>-0.307562</td>      <td>0.619306</td>      <td>-0.047431</td>    </tr>    <tr>      <th>888</th>      <td>0.827377</td>      <td>-1.355574</td>      <td>0.000000</td>      <td>0.432793</td>      <td>2.008933</td>      <td>-0.482043</td>      <td>-0.307562</td>      <td>0.619306</td>      <td>-0.047431</td>    </tr>    <tr>      <th>889</th>      <td>-1.566107</td>      <td>0.737695</td>      <td>-0.284663</td>      <td>-0.474545</td>      <td>-0.473674</td>      <td>2.074505</td>      <td>-0.307562</td>      <td>-1.614710</td>      <td>-0.047431</td>    </tr>    <tr>      <th>890</th>      <td>0.827377</td>      <td>0.737695</td>      <td>0.177063</td>      <td>-0.474545</td>      <td>-0.473674</td>      <td>-0.482043</td>      <td>3.251373</td>      <td>-1.614710</td>      <td>-0.047431</td>    </tr>  </tbody></table><p>891 rows × 9 columns</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># numpy to torch</span></span><br><span class="line">train_X = torch.FloatTensor(train_X[:])</span><br><span class="line">train_Y = torch.LongTensor(train_Y[:])</span><br><span class="line">val_X   = torch.FloatTensor(test_X[:])</span><br><span class="line">val_Y   = torch.LongTensor(test_Y[:])</span><br><span class="line">train_X</span><br></pre></td></tr></table></figure><p>tensor([[ 0.8274,  0.7377, -0.5925,  …, -0.3076,  0.6193, -0.0474],<br>            [-1.5661, -1.3556,  0.6388,  …, -0.3076, -1.6147, -0.0474],<br>            [ 0.8274, -1.3556, -0.2847,  …, -0.3076,  0.6193, -0.0474],<br>            …,<br>            [ 0.8274, -1.3556,  0.0000,  …, -0.3076,  0.6193, -0.0474],<br>            [-1.5661,  0.7377, -0.2847,  …, -0.3076, -1.6147, -0.0474],<br>            [ 0.8274,  0.7377,  0.1771,  …,  3.2514, -1.6147, -0.0474]])</p><hr><h3 id="Modeling"><a href="#Modeling" class="headerlink" title="Modeling"></a>Modeling</h3><ul><li>model</li><li>loss_fn</li><li>optimizer</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ----------------- pytorch ---------------------</span></span><br><span class="line"><span class="comment"># ----------- define Neural Network -------------</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Net</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.fc1 = nn.Linear(<span class="built_in">len</span>(features), <span class="number">256</span>)</span><br><span class="line">        self.fc2 = nn.Linear(<span class="number">256</span>, <span class="number">32</span>)</span><br><span class="line">        self.fc3 = nn.Linear(<span class="number">32</span>, <span class="number">8</span>) </span><br><span class="line">        self.fc4 = nn.Linear(<span class="number">8</span>, <span class="number">2</span>) <span class="comment"># dead or alive</span></span><br><span class="line">        </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = F.relu(self.fc1(x))</span><br><span class="line">        x = F.relu(self.fc2(x))</span><br><span class="line">        x = F.relu(self.fc3(x))</span><br><span class="line">        x = self.fc4(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line">    </span><br><span class="line">model = Net()</span><br><span class="line">model.to(device)</span><br><span class="line">loss_fn = nn.CrossEntropyLoss()</span><br><span class="line">optimizer = torch.optim.SGD(model.parameters(), lr = <span class="number">0.02</span>)</span><br><span class="line"><span class="built_in">print</span>(model,<span class="string">&#x27;\n&#x27;</span>,loss_fn,<span class="string">&#x27;\n&#x27;</span>,optimizer)</span><br></pre></td></tr></table></figure><p>​Net(<br>​      (fc1): Linear(in_features&#x3D;9, out_features&#x3D;256, bias&#x3D;True)<br>​      (fc2): Linear(in_features&#x3D;256, out_features&#x3D;32, bias&#x3D;True)<br>​      (fc3): Linear(in_features&#x3D;32, out_features&#x3D;8, bias&#x3D;True)<br>​      (fc4): Linear(in_features&#x3D;8, out_features&#x3D;2, bias&#x3D;True)<br>​    )<br>​     CrossEntropyLoss()<br>​     SGD (<br>​    Parameter Group 0<br>​        dampening: 0<br>​        differentiable: False<br>​        foreach: None<br>​        lr: 0.02<br>​        maximize: False<br>​        momentum: 0<br>​        nesterov: False<br>​        weight_decay: 0<br>​    )</p><hr><h3 id="Training"><a href="#Training" class="headerlink" title="Training"></a>Training</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ----------- train ------------</span></span><br><span class="line"><span class="comment"># ------ mini-batch SGD --------</span></span><br><span class="line"></span><br><span class="line">batch_size = <span class="number">64</span></span><br><span class="line">batch = <span class="built_in">len</span>(train_X) // batch_size</span><br><span class="line">n_epochs   = <span class="number">800</span></span><br><span class="line"></span><br><span class="line">total_loss = <span class="number">0</span></span><br><span class="line">loop = tqdm(<span class="built_in">range</span>(n_epochs))</span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> loop:</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(batch):</span><br><span class="line">        start = i * batch_size</span><br><span class="line">        end = start + batch_size <span class="keyword">if</span> ((start + batch_size) &gt; <span class="built_in">len</span>(train_X)) <span class="keyword">else</span> <span class="built_in">len</span>(train_X)</span><br><span class="line">        <span class="comment"># Get data to cuda if possible</span></span><br><span class="line">        x_t = train_X[start:end].to(device)</span><br><span class="line">        y_t = train_Y[start:end].to(device)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># forward</span></span><br><span class="line">        output = model(x_t) <span class="comment"># prediction</span></span><br><span class="line">        loss   = loss_fn(output,y_t) <span class="comment"># loss between predictions and answers</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># backward</span></span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line">        </span><br><span class="line">        values, labels = torch.<span class="built_in">max</span>(output, <span class="number">1</span>) <span class="comment"># probability corresponds to label</span></span><br><span class="line">        total_loss += loss.item() * batch_size</span><br><span class="line">        </span><br><span class="line">    total_loss = total_loss / <span class="built_in">len</span>(train_X)</span><br><span class="line">    <span class="comment"># tqdm</span></span><br><span class="line">    loop.set_postfix(loss = <span class="string">&#x27;&#123;:6f&#125;&#x27;</span>.<span class="built_in">format</span>(total_loss))</span><br></pre></td></tr></table></figure><p>100%|██████████| 800&#x2F;800 [00:21&lt;00:00, 37.97it&#x2F;s, loss&#x3D;0.195122]</p><p>模型输出端两个结点，表示两种结果的概率，通过<code>max()</code>方法映射成对应的01标签</p><hr><h3 id="Predictions"><a href="#Predictions" class="headerlink" title="Predictions"></a>Predictions</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    test_result = model(val_X)</span><br><span class="line">labels = torch.<span class="built_in">max</span>(test_result, <span class="number">1</span>)[<span class="number">1</span>]</span><br><span class="line">survived = labels.data.numpy()</span><br><span class="line">labels</span><br></pre></td></tr></table></figure><p>tensor([0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1,<br>            1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0,<br>            1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0,<br>            1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0,<br>            1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1,<br>            1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,<br>            0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0,<br>            1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0,<br>            0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,<br>            0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1,<br>            1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1,<br>            0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0,<br>            0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1,<br>            0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0,<br>            0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1,<br>            0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0,<br>            0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0,<br>            0, 1, 0, 1, 1, 0, 1, 0, 0, 1])</p><hr><h3 id="Submission"><a href="#Submission" class="headerlink" title="Submission"></a>Submission</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">submission = pd.DataFrame(&#123;<span class="string">&#x27;PassengerId&#x27;</span>: sub[<span class="string">&#x27;PassengerId&#x27;</span>], <span class="string">&#x27;Survived&#x27;</span>: survived&#125;)</span><br><span class="line">submission.to_csv(<span class="string">&#x27;submission.csv&#x27;</span>, index=<span class="literal">False</span>)</span><br><span class="line">submission</span><br></pre></td></tr></table></figure><table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th></th>      <th>PassengerId</th>      <th>Survived</th>    </tr>  </thead>  <tbody>    <tr>      <th>0</th>      <td>892</td>      <td>0</td>    </tr>    <tr>      <th>1</th>      <td>893</td>      <td>0</td>    </tr>    <tr>      <th>2</th>      <td>894</td>      <td>0</td>    </tr>    <tr>      <th>3</th>      <td>895</td>      <td>0</td>    </tr>    <tr>      <th>4</th>      <td>896</td>      <td>0</td>    </tr>    <tr>      <th>...</th>      <td>...</td>      <td>...</td>    </tr>    <tr>      <th>413</th>      <td>1305</td>      <td>0</td>    </tr>    <tr>      <th>414</th>      <td>1306</td>      <td>1</td>    </tr>    <tr>      <th>415</th>      <td>1307</td>      <td>0</td>    </tr>    <tr>      <th>416</th>      <td>1308</td>      <td>0</td>    </tr>    <tr>      <th>417</th>      <td>1309</td>      <td>1</td>    </tr>  </tbody></table><p>418 rows × 2 columns</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;Titanic-PyTorch&quot;&gt;&lt;a href=&quot;#Titanic-PyTorch&quot; class=&quot;headerlink&quot; title=&quot;Titanic_PyTorch&quot;&gt;&lt;/a&gt;Titanic_PyTorch&lt;/h1&gt;&lt;p&gt;&lt;a href=&quot;https://w</summary>
      
    
    
    
    <category term="机器学习" scheme="http://example.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="pytorch" scheme="http://example.com/tags/pytorch/"/>
    
    <category term="ML" scheme="http://example.com/tags/ML/"/>
    
    <category term="DNN" scheme="http://example.com/tags/DNN/"/>
    
  </entry>
  
  <entry>
    <title>Games101总结</title>
    <link href="http://example.com/2023/09/12/games101%E6%80%BB%E7%BB%93/"/>
    <id>http://example.com/2023/09/12/games101%E6%80%BB%E7%BB%93/</id>
    <published>2023-09-12T13:46:41.000Z</published>
    <updated>2023-09-13T05:49:22.297Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Games101总结"><a href="#Games101总结" class="headerlink" title="Games101总结"></a>Games101总结</h1><p>时隔两年，当初的很多东西都快遗失了，简单做一个笔记备份，彻底和本科阶段的学习做个告别</p><p>记录从前对图形学的热枕</p><h1 id="01-101"><a href="#01-101" class="headerlink" title="01 - 101"></a>01 - 101</h1><p>光栅化，几何，光线追踪，动画（模拟）</p><p><strong>相关学科</strong></p><ul><li>线性代数，微积分，统计</li><li>光学，力学</li><li>信号处理，数值分析</li><li>美学</li></ul><p><strong>线性代数</strong></p><p>向量点乘</p><p>向量叉乘：右手螺旋定则，例子：已知两轴确定坐标轴，判定左右内外</p><p><code>x×y=z</code>右手坐标系，<code>a×a=0</code>（0向量）</p><p>矩阵</p><p>矩阵*向量 变换</p><p>转置，逆，单位矩阵，伴随矩阵，向量乘法转矩阵形式（点乘和叉乘）</p><hr><h1 id="Games101-Transformation"><a href="#Games101-Transformation" class="headerlink" title="Games101-Transformation"></a>Games101-Transformation</h1><p><strong>提纲</strong></p><ul><li>Modeling</li><li>Viewing</li></ul><p><strong>scale缩放</strong></p><h1 id="left-begin-matrix-x’-y’-end-matrix-right"><a href="#left-begin-matrix-x’-y’-end-matrix-right" class="headerlink" title="$$\left[ \begin{matrix}   x’  \   y’  \end{matrix}\right]"></a>$$<br>\left[<br> \begin{matrix}<br>   x’  \<br>   y’<br>  \end{matrix}<br>\right]</h1><p>\left[<br> \begin{matrix}<br>   s_x &amp; 0 \<br>   0 &amp; s_y<br>  \end{matrix}<br>\right]<br>\left[<br> \begin{matrix}<br>   x  \<br>   y<br>  \end{matrix}<br>\right]<br>$$</p><p>scale matrix</p><p>$$<br>\left[<br> \begin{matrix}<br>   s_x &amp; 0 \<br>   0 &amp; s_y<br>  \end{matrix}<br>\right]<br>$$</p><p><strong>shear错切</strong></p><p><strong>rotate旋转</strong></p><h1 id="left-begin-matrix-x’-y’-end-matrix-right-1"><a href="#left-begin-matrix-x’-y’-end-matrix-right-1" class="headerlink" title="$$\left[ \begin{matrix}   x’  \   y’  \end{matrix}\right]"></a>$$<br>\left[<br> \begin{matrix}<br>   x’  \<br>   y’<br>  \end{matrix}<br>\right]</h1><p>\left[<br> \begin{matrix}<br>   a &amp; b \<br>   c &amp; d<br>  \end{matrix}<br>\right]<br>\left[<br> \begin{matrix}<br>   x  \<br>   y<br>  \end{matrix}<br>\right]<br>$$</p><p><strong>平移</strong></p><p>不是线性变换</p><p><strong>齐次坐标</strong></p><p>$$<br>point&#x3D;(x,y,1)^T<br>\<br>vector&#x3D;(x,y,0)<br>$$</p><p>向量平移不变性</p><ul><li>逆变换：逆矩阵</li><li>变换的顺序很重要，矩阵乘法的顺序不满足交换律</li><li>多个变换，矩阵往左附加，n个变换矩阵可以合成一个矩阵以表示一个复杂变换</li><li>变换分解，</li></ul><p>3D Transformations</p><p>$$<br>point&#x3D;(x,y,z,1)^T<br>\<br>vector&#x3D;(x,y,z,0)<br>\<br>$$</p><p><strong>前置知识</strong></p><p>正交矩阵</p><p>$$<br>A^T &#x3D; A^{-1}<br>$$</p><p>旋转矩阵满足等式   $ R_{-θ}&#x3D;R_θ^T&#x3D;R_θ^{-1} $</p><p><strong>3D Transformation</strong></p><p>对二维矩阵扩展，加入 z 轴（或理解成加入一个维度），例子如下</p><p>Scale Matrix：$\left[\begin{matrix}S_x &amp; 0 &amp; 0 &amp; 0 \0&amp;S_y&amp;0&amp;0\0&amp;0&amp;S_z&amp;0\0&amp;0&amp;0&amp;1\ \end{matrix}\right]$</p><p>Translation：$\left[\begin{matrix}1&amp;0&amp;0&amp;t_x\0&amp;1&amp;0&amp;t_y\0&amp;0&amp;1&amp;t_z\0&amp;0&amp;0&amp;1\ \end{matrix}\right]$</p><p><strong>3D Rotation</strong></p><ul><li>绕轴旋转</li></ul><p>$$\left[\begin{matrix}1&amp;0&amp;0&amp;0\0&amp;cosθ&amp;-sinθ&amp;0\0&amp;sinθ&amp;cosθ&amp;0\0&amp;0&amp;0&amp;1\ \end{matrix}\right]$$</p><p>$$\left[\begin{matrix}cosθ&amp;0&amp;sinθ&amp;0\0&amp;1&amp;0&amp;0\-sinθ&amp;0&amp;cosθ&amp;0\0&amp;0&amp;0&amp;1\ \end{matrix}\right]$$</p><p>$$\left[\begin{matrix}cosθ&amp;-sinθ&amp;0&amp;0\sinθ&amp;cosθ&amp;0&amp;0\0&amp;0&amp;1&amp;0\0&amp;0&amp;0&amp;1\ \end{matrix}\right]$$</p><p>对任何3D旋转：$R_{xyz}(\alpha,\beta,\gamma)&#x3D;R_x(\alpha)R_y(\beta)R_z(\gamma)$</p><p><strong>MVP变换</strong></p><ul><li>modeling</li><li>viewing</li><li>projection</li></ul><p><strong>Viewing Transformation</strong></p><p>- </p><p>3个参数 $\vec e$</p><p>固定相机位置</p><p>任意位置相机移动至**原点 (0,0,0)<strong>，方向朝</strong><code>-z</code>**方向，角度对 <strong>Y 轴</strong></p><p>先平移$\left[\begin{matrix}1&amp;0&amp;0&amp;-X_e\0&amp;1&amp;0&amp;-Y_e\0&amp;0&amp;1&amp;-Z_e\0&amp;0&amp;0&amp;1\ \end{matrix}\right]$</p><p>再旋转（旋转矩阵如何表示？）</p><ul><li>当前位置直接求矩阵很困难</li><li>利用旋转矩阵的正交性</li><li>求出坐标系到的当前旋转角度的旋转矩阵M</li><li>$M^{-1}&#x3D;M^T$</li></ul><p><strong>Projection Transformation</strong></p><p>立方体：<code>[l,r]*[b,t]*[f,n]</code>（左右，上下，前后，对相机来说n &gt; f）</p><p><strong>正交变换（Orthographic Transformation）</strong></p><p>投影到$[-1,1]^3$范围内，本质是将任意位置的物体，表示在固定距离角度的空间或平面上</p><p>通过平移+放缩实现，立方体为例</p><ul><li>几何中心移动至原点</li><li>放缩至$[-1,1]^3$</li></ul><p>$M_{ortho}&#x3D;\left[\begin{matrix}\frac{2}{r-l}&amp;0&amp;0&amp;0\0&amp;\frac{2}{t-b}&amp;0&amp;0\0&amp;0&amp;\frac{2}{n-f}&amp;0\0&amp;0&amp;0&amp;1\ \end{matrix}\right]\left[\begin{matrix}1&amp;0&amp;0&amp;-\frac{l+r}{2}\0&amp;1&amp;0&amp;-\frac{b+t}{2}\0&amp;0&amp;1&amp;-\frac{n+f}{2}\0&amp;0&amp;0&amp;1\ \end{matrix}\right]$</p><ul><li>注意<code>n-f</code></li></ul><p><strong>透视变换（Perspective Transformation）</strong></p><p>···</p><hr><h1 id="shadows，光栅化补充"><a href="#shadows，光栅化补充" class="headerlink" title="shadows，光栅化补充"></a>shadows，光栅化补充</h1><p>阴影映射 | 阴影图</p><p>shadow mapping</p><p>不在阴影里的点：相机能看到，光源也能看到</p><p>在阴影里的点：只有相机能看到</p><p>只讨论点光源</p><p>硬阴影：阴影边缘非常锐利，点光源</p><hr><ol><li>从光源看向场景，记录看到的任何点的深度</li><li>从相机看向场景，同上，然后投影回光源，判断是否能同时被看到</li></ol><p>软阴影：边缘逐渐消失，光源有一定的大小</p><hr><h1 id="Games101-Geometry"><a href="#Games101-Geometry" class="headerlink" title="Games101 Geometry"></a>Games101 Geometry</h1><h3 id="纹理续"><a href="#纹理续" class="headerlink" title="纹理续"></a>纹理续</h3><p>现代GPU中，纹理&#x3D;内存+范围查询（滤波），即可以理解为一块数据</p><h4 id="环境光（Environmental-Lighting）"><a href="#环境光（Environmental-Lighting）" class="headerlink" title="环境光（Environmental Lighting）"></a>环境光（Environmental Lighting）</h4><p>可以用纹理描述环境光，并拿去渲染表示环境光的效果</p><p>但是现实中随着人的走动光照位置会发生变化，所以上述方式是不考虑深度信息的近似方法</p><h4 id="Spherical-Environmental-Map"><a href="#Spherical-Environmental-Map" class="headerlink" title="Spherical Environmental Map"></a>Spherical Environmental Map</h4><p>将环境光记录在球体中，再展开</p><p>扭曲问题：参考世界地图</p><h4 id="Cube-Map"><a href="#Cube-Map" class="headerlink" title="Cube Map"></a>Cube Map</h4><p>将球之于一个立方体中，从球心向外做投影，得到6张图，解决扭曲</p><h4 id="凹凸贴图"><a href="#凹凸贴图" class="headerlink" title="凹凸贴图"></a>凹凸贴图</h4><p>纹理定义法线方向的相对高度</p><p>在不改变几何信息的情况下，可以通过复杂的纹理来表示复杂的结构</p><p>相对高度变化-法线变化-着色变化，产生明暗变化</p><p>由于没有改变几何，所以物体边缘会暴露（很光滑）</p><h4 id="位移贴图"><a href="#位移贴图" class="headerlink" title="位移贴图"></a>位移贴图</h4><p>真的移动顶点位置，</p><p>描述几何体的三角形要足够多</p><h4 id="3D贴图"><a href="#3D贴图" class="headerlink" title="3D贴图"></a>3D贴图</h4><p>定义三维空间中的一个任意点</p><p>通过噪声函数生成空间中任意点的数据</p><h4 id="3D纹理和体积渲染"><a href="#3D纹理和体积渲染" class="headerlink" title="3D纹理和体积渲染"></a>3D纹理和体积渲染</h4><p>医学</p><h3 id="Geometry"><a href="#Geometry" class="headerlink" title="Geometry"></a>Geometry</h3><p>隐式，显式</p><p>隐式表示：</p><p>方程，难看出表示什么，但可以方便的判断是否在表面上</p><p>显式：直接或参数映射</p><p>参数映射，二维映射到三维</p><h4 id="CSG（Constructive-Solid-Geometry）"><a href="#CSG（Constructive-Solid-Geometry）" class="headerlink" title="CSG（Constructive Solid Geometry）"></a>CSG（Constructive Solid Geometry）</h4><p>通过基本几何体的变换，布尔计算，得到复杂几何体</p><h4 id="Distance-Function"><a href="#Distance-Function" class="headerlink" title="Distance Function"></a>Distance Function</h4><p> 代替布尔运算，通常使用<strong>距离函数</strong>使表面融合</p><p>空间中的任意一点到物体表面的任意一点的最小距离？（理解不能）</p><p>蜗牛，蘑菇，瓢虫等等都可以用距离函数表示，没有几何形体，一种隐式表示</p><p>分离物体：距离函数为0的就是物体表面（水平集level sets）</p><p><strong>分形</strong>Fractals</p><p>渲染时会引起强烈的走样，因为变化频率太高</p><hr><h1 id="相机，透镜，光场"><a href="#相机，透镜，光场" class="headerlink" title="相机，透镜，光场"></a>相机，透镜，光场</h1><p><strong>针孔摄像机</strong>：无法记录深度信息，任意位置都是锐利的，没有虚化</p><p><strong>视场（FOV）</strong>：成像面的长度h和透镜的距离f<br>$$<br>FOV&#x3D;2arctan(\frac{h}{2f})<br>$$<br>市场上焦距和FOV参数以35mm大小的胶片为标准</p><hr><p><strong>Exposure曝光度</strong></p><p>H&#x3D;T*E，T由快门控制，E由光圈大小，ISO增益等决定</p><p>time*irradiance</p><p>ISO不但增强了有效信息还放大了噪声信息</p><hr><p>lens </p><p>假设对于一个透镜，其焦距可以任意修改<br>$$<br>物距：z_o \<br>像距：z_i\<br>\frac{1}{f}&#x3D;\frac{1}{z_i}+\frac{1}{z_o}<br>$$</p><hr><p>Circle of Confusion（CoC）Size</p><p>感光元件在焦点后面，会投射在一个圆内，A就是光圈大小，A越大，C越大模糊</p><hr><p>可以模拟薄透镜来实现景深效果</p><p><strong>Light Field &#x2F; Lumigraph</strong></p><p>任意位置任意方向的光</p><hr><h1 id="Materials-and-Appearance"><a href="#Materials-and-Appearance" class="headerlink" title="Materials and Appearance"></a>Materials and Appearance</h1><p>由渲染方程可知，BRDF决定了材质</p><p>透明、半透明材质，考虑反射和折射</p><p>光线得折射反射，使能量发生变化从而使某些材质显示出一定的颜色</p><hr><p><strong>折射</strong></p><p>BTDF</p><p>还是用几何光学分析</p><p>水底条状纹理，聚焦，光线正好聚焦到同一点</p><p>斯涅耳定律，折射定律</p><p>不同材料的折射率不同</p><hr><p>入射介质和出射介质，折射角可能无意义，出现全反射现象</p><p>斯涅尔窗</p><hr><p>BSTF（散射）&#x3D;BRDF（反射）+BTDF（折射）</p><hr><p><strong>菲涅尔项</strong></p><p>不同角度的反射能量不同</p><p>光和物体越平行，越多能量被反射</p><p>金属材质的菲涅尔项在垂直使仍然能达到0.9+</p><hr><p><strong>微表面模型</strong></p><p>物体表面粗糙，但当距离足够远时，就是材质和外观，就如墙面忽略了其表面微小的凹 凸</p><hr><p>各向同性：微表面分布基本均匀</p><p>各向异性：微表面的法线分布有明确的方向性（各种人造金属制品，锅，光盘，尼龙等）</p><hr><h1 id="GAMES101-Geometry（Curves-and-Surfaces）"><a href="#GAMES101-Geometry（Curves-and-Surfaces）" class="headerlink" title="GAMES101 - Geometry（Curves and Surfaces）"></a>GAMES101 - Geometry（Curves and Surfaces）</h1><p><strong>Point Cloud</strong></p><p>大力出奇迹</p><p>只要点够密，可以表示任何东西</p><p>难点：如何将一堆点表示表面</p><p><strong>Polygon Mesh</strong></p><p>多边形网格，最广泛使用</p><p>难点：如何组合多边形</p><p>一堆点v，一堆法线vn，一堆纹理坐标vt存到文本文件，</p><p>一个立方体8个点，6个面6个法线，存储时会去除冗余</p><p><code>f 5/1/1 1/2/1 4/3/1</code>，三个点确定一个三角形，共用一条法线</p><p><code>点/纹理坐标/法线</code></p><hr><p><strong>Bezier Curve</strong></p><p>每条线段看作一条路径，假定可以控制时间t，找出时间t所处的点</p><p>任意一个时间t都可以找出对应的点</p><p>通过点的位置和时间t能找出所求点的代数表示</p><p>三维空间同样适用</p><p>仿射不变性：仿射变换下，只要对控制点做仿射变换</p><p>凸包性质：Bezier曲线一定在控制点形成的凸包（最小的凸多边形）内</p><hr><p>控制点过多就不容易控制曲线形状，且动一个点整条曲线都会变化</p><p>逐段，一般每段3阶4个点，绘图软件钢笔工具</p><p>C<sup>0</sup>连续，几何上接触</p><p>C<sup>1</sup>连续，一阶导数相等，光滑</p><p>C<sup>2</sup>连续，二阶导数…</p><p>如何保证曲线连接处光滑？切线导数相同，控制点距离一样</p><p><strong>spline样条</strong></p><p>可控曲线</p><p><strong>B-splines</strong></p><p>由不同的基函数控制…</p><p>Bezier曲线的扩展，有局部性</p><p><strong>NURBS</strong></p><p>对B样条的进一步扩展</p><hr><p><strong>Bezier Surfaces</strong></p><p>两个方向分别运用Bezier Curve，曲线扫过的空间就形成一个曲面</p><hr><h1 id="GAMES101-曲面细分"><a href="#GAMES101-曲面细分" class="headerlink" title="GAMES101 - 曲面细分"></a>GAMES101 - 曲面细分</h1><p><strong>细分Mash</strong></p><p>引入更多三角形，同时位置发生变化使之更加光滑</p><p><strong>loop细分</strong></p><p>只能用于三角形网格</p><p><code>(1-n*u)*原位置+u*领近点位置平均</code></p><p>n:顶点的度</p><p>u:权？</p><p><strong>Catmull-Clark subdivision</strong></p><p>适用于任意形状的网格</p><p>术语：四边形面，非四边形面，奇异点（度不为4的点）</p><p>将边上的中点和面的中点都连起来，变为都是四边形的网格（所以一次细分后奇异点不再增加），奇异点增加（非四边形面中心点一定是奇异点）</p><hr><p>不同场合选择合适数量的网格</p><hr><p><strong>网格简化</strong></p><p><strong>边坍缩</strong></p><p>边变顶点</p><p><strong>二次误差度量</strong></p><p>最小化二次误差，将点放置到合适的位置，使得对原图的影响最小</p><p>从二次误差度量最小的边开始坍缩，优先队列或堆，贪心算法（局部最优）</p><hr><h1 id="Animation"><a href="#Animation" class="headerlink" title="Animation"></a>Animation</h1><p>早期关键帧技术</p><p><strong>基于物理的动画</strong></p><p>Mass Spring System，质点弹簧系统</p><p>FEM,有限元</p><hr><p><strong>Particle Systems 粒子系统</strong></p><p>将粒子一个个的定义出来，设定粒子间的相互作用力，引力和斥力，粒子间的相互作用力符合什么规律最后表现的就想什么</p><p>通过一些函数，表现粒子突然生成，突然消失，定义粒子的速度和位置 </p><p>包括流体，烟云等都可以模拟</p><hr><p><strong>运动学</strong></p><p>骨骼</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;Games101总结&quot;&gt;&lt;a href=&quot;#Games101总结&quot; class=&quot;headerlink&quot; title=&quot;Games101总结&quot;&gt;&lt;/a&gt;Games101总结&lt;/h1&gt;&lt;p&gt;时隔两年，当初的很多东西都快遗失了，简单做一个笔记备份，彻底和本科阶段的学习</summary>
      
    
    
    
    <category term="图形学" scheme="http://example.com/categories/%E5%9B%BE%E5%BD%A2%E5%AD%A6/"/>
    
    
    <category term="Graphics" scheme="http://example.com/tags/Graphics/"/>
    
  </entry>
  
  <entry>
    <title>SGI STL内存配置器分析</title>
    <link href="http://example.com/2021/12/11/SGI%20STL%E5%86%85%E5%AD%98%E9%85%8D%E7%BD%AE%E5%99%A8%E5%88%86%E6%9E%90/"/>
    <id>http://example.com/2021/12/11/SGI%20STL%E5%86%85%E5%AD%98%E9%85%8D%E7%BD%AE%E5%99%A8%E5%88%86%E6%9E%90/</id>
    <published>2021-12-11T12:04:00.000Z</published>
    <updated>2023-09-13T05:52:32.001Z</updated>
    
    <content type="html"><![CDATA[<p>stl的容器利用缺省的Alloc参数为自己分配内存，Alloc就是stl的空间配置器，用于空间的管理</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">class</span> <span class="title class_">T</span>, <span class="keyword">class</span> <span class="title class_">Alloc</span> = alloc&gt;</span><br><span class="line"><span class="keyword">class</span> vector &#123;&#125;</span><br></pre></td></tr></table></figure><p>空间配置器由<strong>内存池</strong>，<strong>链表</strong>来具体实现对空间的<strong>分配</strong>、<strong>回收</strong>和<strong>管理</strong></p><hr><p>在一个对象的创建和销毁时(<code>new</code>和<code>delete</code>)，各需要经历了两个步骤</p><p>创建</p><ul><li>分配空间</li><li>用构造函数构造对象</li></ul><p>销毁</p><ul><li>析构</li><li>释放空间</li></ul><p>注意区分<code>::operator new</code>和<code>placement new</code></p><hr><p>进一步抽象，就像房子和内部物体结构，同样可以在上面的步骤中，将空间管理和具体结构管理分开</p><p>见框架图</p><hr><p>构造：调用<code>placement new</code>在指定的位置上设定初值</p><p>析构：析构指定位置的东西</p><hr><h3 id="空间的管理"><a href="#空间的管理" class="headerlink" title="空间的管理"></a>空间的管理</h3><p>使用一块内存而不加管理会导致很多问题，诸如向系统堆栈（system heap）请求空间，内存不足，内存碎片等</p><p>针对内存碎片，有<strong>双层级配置器</strong></p><p>对于内存不足和请求system heap，有<strong>内存池</strong>（memory pool）</p><hr><p>为什么不只用一级配置器，malloc似乎也可以分配小空间？</p><p>防止过多的额外负担（overhead），每向系统申请一块内存，就有一部分其他的空间被系统拿来管理我们申请的那片内存，申请内存越小，系统管理所用空间占比就越大，举例就是去离家很近的地方，选择走路而不是开车，走路不用花钱，开车却有一定的花费，这个花费对这点距离来说没有必要，但是出远门，开车肯定比走路好，这些花费是必要的</p><hr><p>总结做了一个框架图来展示SGI alloc的实现</p><p><img src="/iimmgg/sgialloc.png" alt="关系图"></p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;stl的容器利用缺省的Alloc参数为自己分配内存，Alloc就是stl的空间配置器，用于空间的管理&lt;/p&gt;
&lt;figure class=&quot;highlight c++&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line</summary>
      
    
    
    
    <category term="C++" scheme="http://example.com/categories/C/"/>
    
    
    <category term="C++" scheme="http://example.com/tags/C/"/>
    
  </entry>
  
  <entry>
    <title>Inside The C++ Object Model简单记录</title>
    <link href="http://example.com/2021/11/25/Inside_C++_Obj_Model/"/>
    <id>http://example.com/2021/11/25/Inside_C++_Obj_Model/</id>
    <published>2021-11-25T04:05:00.000Z</published>
    <updated>2023-09-12T10:23:33.571Z</updated>
    
    <content type="html"><![CDATA[<p>此篇为阅读《Inside The C++ Object Model》时对其中相对重要的 data 语义和 function 语义的一些记录</p><hr><h2 id="data语义"><a href="#data语义" class="headerlink" title="data语义"></a>data语义</h2><ul><li><p>编译器一般将多个access sections连锁在一起形成一个区块，这个操作不会降低效率，诸如多个public域</p></li><li><p>C\C++的边界调整有可能会在中间插入若干bit（类似c的结构体内存对齐）</p></li><li><p>编译器会自动生成一些内容以支撑对象例如vptr，一般vptr会被插入对象的开头或结尾，依赖编译器的处理</p></li></ul><hr><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// static data member 不从属于class</span></span><br><span class="line"><span class="comment">// 对其的引用会得到一个指向其数据类型的指针,如下</span></span><br><span class="line"><span class="keyword">struct</span> <span class="title class_">Test</span> &#123;</span><br><span class="line"><span class="type">static</span> <span class="type">const</span> <span class="type">int</span> st_con = <span class="number">10</span>;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line"><span class="keyword">auto</span> pt = &amp;Test::st_con; <span class="comment">//pt is const int*</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>所以这时，当两个class都有同名static data member则会产生冲突，此时编译器为每个static data member编码（<strong>name-mangling</strong>），用以区分彼此</p><hr><p>nonstatic data member在存取时和C的效率没什么两样，是C++从C中借鉴过来的一部分</p><p>存取需要通过明指或暗指（this）</p><hr><p>但当用<strong>指针</strong>且继承结构中存在<strong>虚拟继承</strong>时就无法在编译阶段确定成员属于何对象（虚拟继承不常用，可忽略）</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">object.x;</span><br><span class="line">pt-&gt;x;<span class="comment">//pt指向需要在执行阶段才能判断</span></span><br></pre></td></tr></table></figure><hr><p>派生类赋值给基类(但vs似乎避免了这个情况？)，会将非继承成员放到基类因内存对齐而填充的空间中，此时基类结构发生改变，这时当另一个基类给当前基类赋值时，基类结构中的子类成员会被未知数据填充</p><hr><p>vptr在尾部就兼容C，在头部更有利于多继承，vs中vptr被插入至头部，例子见下</p><p>单继承体系中把一个派生类地址给基类指针是一个自然过程，但在vptr在头部且派生类中含有虚函数时就需要编译器介入，多继承+虚拟继承就更需要了</p><p>多重继承，在指针变化时内部会进行计算以获得目标基类的offset(编译器介入)</p><p>编译器优化之后，封装不会对执行期效率产生什么影响</p><h2 id="function-语意"><a href="#function-语意" class="headerlink" title="function 语意"></a>function 语意</h2><p>类成员函数有3种状态：<code>static, nonstatic, vitual</code></p><p>类内函数成员在编译器的优化后可以获得不低于外部函数的效率，会被编译器优化成了类外部函数实体</p><hr><p><strong>vitual member function</strong></p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 用指针对虚函数的调用</span></span><br><span class="line">ptr-&gt;<span class="built_in">fun</span>();</span><br><span class="line"><span class="comment">// 会转化为</span></span><br><span class="line">(*ptr-&gt;vptr[<span class="number">1</span>])(ptr);<span class="comment">// 通过ptr获取虚表中的函数指针，通过函数指针调用虚函数</span></span><br></pre></td></tr></table></figure><p><strong>C++多态：以一个基类指针或引用寻址出一个派生类对象</strong></p><p>如何在执行期确定虚函数的实体？</p><ul><li>具体是哪个类（指针或引用指向的真实类型）</li><li>哪个虚函数（虚函数地址）</li></ul><p>如何存放两个需要的信息？</p><ul><li>一个由编译器提供的vptr指针，指向vitual table，其中存储了type_info for object和 虚函数的执行期地址</li><li>为了找到函数地址，每个虚函数被指定一个索引（纯虚函数同样有自己的索引</li></ul><p>在编译时虚函数由其对象调用可知，由编译器完全控制，唯一一个在执行期才知道的消息是：slot指向的函数实体</p><p><strong>指针的类型是定义时确定的，但指向的对象的类型不确定</strong>，诸如：<code>base *p = &amp;child;</code>其中p是基类指针，但可以指向派生类对象，对象信息存放在virtual table中的RTT字段，运行时确定</p><hr><ul><li><p>非成员函数，成员函数、静态函数被编译器优化成<strong>完全相同</strong>的形式，函数效率相同</p></li><li><p>加上vitual后，随继承结构复杂度耗时增加</p></li></ul><hr><p><strong>普通成员函数</strong></p><p>同数据成员一样，直接是在内存中的真正地址，需要依赖于对象访问</p><hr><p><strong>inline</strong></p><p>是**#define的一种安全替代品**，但仍然需要被小心处理，过多的参数（产生临时变量）和嵌套会导致扩展码大量增加或无法扩展开来</p><p>inline函数有惊人的效率，被视作不变表达式，编译器将其提出至循环之外，只计算一次</p><hr><p><strong>类结构实例研究</strong></p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">struct</span> <span class="title class_">Test</span> &#123;</span><br><span class="line"><span class="built_in">Test</span>(<span class="type">int</span> _a, <span class="type">char</span> _b) : <span class="built_in">a</span>(_a) , <span class="built_in">b</span>(_b) &#123; &#125;;</span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">fun</span><span class="params">()</span> </span>&#123;&#125;;</span><br><span class="line"><span class="function"><span class="keyword">virtual</span> <span class="type">void</span> <span class="title">vir_fun_0</span><span class="params">()</span> </span>&#123;&#125;;<span class="comment">//有一个4B的指针</span></span><br><span class="line"><span class="function"><span class="keyword">virtual</span> <span class="type">void</span> <span class="title">vir_fun_1</span><span class="params">()</span> </span>&#123;&#125;;</span><br><span class="line"><span class="type">int</span> a;<span class="comment">//4B</span></span><br><span class="line"><span class="type">char</span> b;<span class="comment">//1B</span></span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="keyword">struct</span> <span class="title class_">TT</span> : <span class="keyword">public</span> Test &#123;</span><br><span class="line"><span class="type">char</span> c;</span><br><span class="line"><span class="built_in">TT</span>(<span class="type">int</span> _a, <span class="type">char</span> _b, <span class="type">char</span> _c) : <span class="built_in">Test</span>(_a, _b), <span class="built_in">c</span>(_c) &#123;&#125;;</span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">vir_fun_1</span><span class="params">()</span> </span>&#123;&#125;;<span class="comment">//override</span></span><br><span class="line"><span class="function"><span class="keyword">virtual</span> <span class="type">void</span> <span class="title">TT_fun</span><span class="params">()</span> </span>&#123;&#125;;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><p>基类与派生类结构如下</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//-----vs编译后的内存布局-----//</span></span><br><span class="line"></span><br><span class="line"><span class="number">1</span>&gt;<span class="function"><span class="keyword">class</span> Test<span class="title">size</span><span class="params">(<span class="number">12</span>)</span>:// 基类,<span class="number">12</span>字节</span></span><br><span class="line"><span class="function"><span class="number">1</span>&gt;+---</span></span><br><span class="line"><span class="function"><span class="number">1</span>&gt; <span class="number">0</span>| &#123;</span>vfptr&#125;<span class="comment">// 4字节指针,指向虚表 ;且位于头部，不兼容C</span></span><br><span class="line"><span class="number">1</span>&gt; <span class="number">4</span>| a    <span class="comment">// int,4字节</span></span><br><span class="line"><span class="number">1</span>&gt; <span class="number">8</span>| b    <span class="comment">// char,1字节</span></span><br><span class="line"><span class="number">1</span>&gt;  | &lt;alignment member&gt; (size=<span class="number">3</span>)<span class="comment">//内存对齐</span></span><br><span class="line"><span class="number">1</span>&gt;+---</span><br><span class="line"><span class="number">1</span>&gt;Test::$vftable@:<span class="comment">//虚表结构</span></span><br><span class="line"><span class="number">1</span>&gt;| &amp;Test_meta<span class="comment">//用于支持RTTI的类型信息</span></span><br><span class="line"><span class="number">1</span>&gt;|  <span class="number">0</span></span><br><span class="line"><span class="number">1</span>&gt; <span class="number">0</span>| &amp;Test::vir_fun_0<span class="comment">//索引 0</span></span><br><span class="line"><span class="number">1</span>&gt; <span class="number">1</span>| &amp;Test::vir_fun_1<span class="comment">//索引 1</span></span><br><span class="line"><span class="number">1</span>&gt;Test::vir_fun_0 <span class="keyword">this</span> adjustor: <span class="number">0</span><span class="comment">//调节器：用于多重继承中,获取夹在第一基类和派生类中间的基类开始地址</span></span><br><span class="line"><span class="number">1</span>&gt;Test::vir_fun_1 <span class="keyword">this</span> adjustor: <span class="number">0</span><span class="comment">//就是一个偏移量，如果中间还有一个基类,则第二基类开始在12字节后（Test占12字节）</span></span><br><span class="line"><span class="number">1</span>&gt;<span class="function"><span class="keyword">class</span> TT<span class="title">size</span><span class="params">(<span class="number">16</span>)</span>://派生类，<span class="number">16</span>字节 =</span> 基类<span class="number">12B</span> + <span class="built_in">sizeof</span>(<span class="type">char</span>) + 内存对齐<span class="number">3</span>字节</span><br><span class="line"><span class="number">1</span>&gt;+---</span><br><span class="line"><span class="number">1</span>&gt; <span class="number">0</span>| +--- (base <span class="keyword">class</span> Test)<span class="comment">//基类保持其结构不变,用于内存对齐的空间仍然存在</span></span><br><span class="line"><span class="number">1</span>&gt; <span class="number">0</span>| | &#123;vfptr&#125;   <span class="comment">//派生类继续使用基类的虚表指针，从此可得vptr在头部有利于OOP的继承机构</span></span><br><span class="line"><span class="number">1</span>&gt; <span class="number">4</span>| | a</span><br><span class="line"><span class="number">1</span>&gt; <span class="number">8</span>| | b</span><br><span class="line"><span class="number">1</span>&gt;  | | &lt;alignment member&gt; (size=<span class="number">3</span>)<span class="comment">//基类中的内存对齐空间任然存在</span></span><br><span class="line"><span class="number">1</span>&gt;| +---</span><br><span class="line"><span class="number">1</span>&gt;<span class="number">12</span>| c<span class="comment">//子类数据成员</span></span><br><span class="line"><span class="number">1</span>&gt;  | &lt;alignment member&gt; (size=<span class="number">3</span>)<span class="comment">//内存对齐</span></span><br><span class="line"><span class="number">1</span>&gt;+---</span><br><span class="line"><span class="number">1</span>&gt;TT::$vftable@:</span><br><span class="line"><span class="number">1</span>&gt;| &amp;TT_meta</span><br><span class="line"><span class="number">1</span>&gt;|  <span class="number">0</span></span><br><span class="line"><span class="number">1</span>&gt; <span class="number">0</span>| &amp;Test::vir_fun_0<span class="comment">//索引0,派生类没有处理,直接继承基类虚函数</span></span><br><span class="line"><span class="number">1</span>&gt; <span class="number">1</span>| &amp;TT::vir_fun_1<span class="comment">//索引1,基类虚函数被派生类override</span></span><br><span class="line"><span class="number">1</span>&gt; <span class="number">2</span>| &amp;TT::TT_fun<span class="comment">//索引2,派生类新的虚函数，表被扩展</span></span><br><span class="line"><span class="number">1</span>&gt;TT::vir_fun_1 <span class="keyword">this</span> adjustor: <span class="number">0</span></span><br><span class="line"><span class="number">1</span>&gt;TT::TT_fun <span class="keyword">this</span> adjustor: <span class="number">0</span></span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;此篇为阅读《Inside The C++ Object Model》时对其中相对重要的 data 语义和 function 语义的一些记录&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&quot;data语义&quot;&gt;&lt;a href=&quot;#data语义&quot; class=&quot;headerlink&quot; title</summary>
      
    
    
    
    <category term="C++" scheme="http://example.com/categories/C/"/>
    
    
    <category term="C++" scheme="http://example.com/tags/C/"/>
    
  </entry>
  
  <entry>
    <title>SpeedTree导入UE后花叶丢失问题解决</title>
    <link href="http://example.com/2021/10/28/speedtree/"/>
    <id>http://example.com/2021/10/28/speedtree/</id>
    <published>2021-10-28T08:21:19.000Z</published>
    <updated>2023-09-13T05:56:05.501Z</updated>
    
    <content type="html"><![CDATA[<p>版本问题，speedtree7的树库导入speedtree8后会出bug，据说会在下一个版本解决</p><p>解决方法就是ue4中<strong>把贴图的alpha通道直接连接到opacity</strong></p><hr><p>通过修改</p><p><img src="/iimmgg/20211028-162845.jpg" alt="20211028-162845"></p><p>修改成：</p><p><img src="/iimmgg/20211028-162902.jpg" alt="20211028-162902"></p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;版本问题，speedtree7的树库导入speedtree8后会出bug，据说会在下一个版本解决&lt;/p&gt;
&lt;p&gt;解决方法就是ue4中&lt;strong&gt;把贴图的alpha通道直接连接到opacity&lt;/strong&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;通过修改&lt;/p&gt;
&lt;p&gt;&lt;img s</summary>
      
    
    
    
    <category term="图形学" scheme="http://example.com/categories/%E5%9B%BE%E5%BD%A2%E5%AD%A6/"/>
    
    
    <category term="ue4" scheme="http://example.com/tags/ue4/"/>
    
  </entry>
  
  <entry>
    <title>C++高级宏操作</title>
    <link href="http://example.com/2021/10/14/C++%E9%AB%98%E7%BA%A7%E5%AE%8F%E6%93%8D%E4%BD%9C/"/>
    <id>http://example.com/2021/10/14/C++%E9%AB%98%E7%BA%A7%E5%AE%8F%E6%93%8D%E4%BD%9C/</id>
    <published>2021-10-14T14:30:00.000Z</published>
    <updated>2023-09-13T05:59:20.656Z</updated>
    
    <content type="html"><![CDATA[<h1 id="C-高级宏操作"><a href="#C-高级宏操作" class="headerlink" title="C++高级宏操作"></a>C++高级宏操作</h1><p>最近在看一个c++各数据类型和 JSON 或 xml 互转的库，重点阅读了其中宏的编写</p><p><a href="https://github.com/xyz347/xpack">xpack项目链接</a></p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 结构体格式如下</span></span><br><span class="line"><span class="keyword">struct</span> <span class="title class_">User</span>&#123;</span><br><span class="line">    <span class="type">long</span> uid;</span><br><span class="line">    string name;</span><br><span class="line">    <span class="built_in">XPACK</span>(<span class="built_in">A</span>(uid, <span class="string">&quot;id&quot;</span>), <span class="built_in">O</span>(name));<span class="comment">//用于指示各变量的特殊操作</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">define</span> XPACK(...)   \</span></span><br><span class="line"><span class="meta">    X_PACK_COMMON    \</span></span><br><span class="line"><span class="meta">    X_PACK_DECODE_BEGIN X_PACK_N(X_PACK_L1, X_PACK_L1_DECODE, __VA_ARGS__) &#125;  \</span></span><br><span class="line"><span class="meta">    X_PACK_ENCODE_BEGIN X_PACK_N(X_PACK_L1, X_PACK_L1_ENCODE, __VA_ARGS__) &#125;</span></span><br></pre></td></tr></table></figure><p>结构体中的变量数目不定，采用可变参数宏：</p><p><code>...</code>和<code>__VA_ARGS__</code>配合使用，<code>...</code>处填充的内容将填充到<code>___VA_ARGS__</code>处</p><p><code>__VA_ARGS__</code>替换最后一个具体参数后所有内容包括逗号等</p><p>当然c++并不提倡使用可变参数</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">define</span> X_PACK_COMMON \</span></span><br><span class="line"><span class="meta">public:               \</span></span><br><span class="line"><span class="meta">    static bool const __x_pack_value = true;</span></span><br></pre></td></tr></table></figure><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">define</span> X_PACK_DECODE_BEGIN                         \</span></span><br><span class="line"><span class="meta">    template<span class="string">&lt;class __X_PACK_DOC, class __X_PACK_ME&gt;</span> \</span></span><br><span class="line"><span class="meta">    void __x_pack_decode(__X_PACK_DOC&amp; __x_pack_obj, __X_PACK_ME &amp;__x_pack_self, const xpack::Extend *__x_pack_extp) &#123;(void)__x_pack_extp;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// encode function</span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> X_PACK_ENCODE_BEGIN                          \</span></span><br><span class="line"><span class="meta">    template <span class="string">&lt;class __X_PACK_DOC, class __X_PACK_ME&gt;</span> \</span></span><br><span class="line"><span class="meta">    void __x_pack_encode(__X_PACK_DOC&amp; __x_pack_obj, const __X_PACK_ME &amp;__x_pack_self, const xpack::Extend *__x_pack_extp) const &#123;(void)__x_pack_extp;</span></span><br><span class="line"></span><br></pre></td></tr></table></figure><p>从这里开始可以看出XPACK已经被展开成了一个静态常量和两个函数，<code>x_PACK_N()</code>被包在函数内部</p><hr><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">define</span> X_PACK_N(LEVEL, ACTION, ...)  X_PACK_COUNT(LEVEL, ACTION, __VA_ARGS__, _99,_98,_97,_96,_95,_94,_93,_92,_91,_90,_89,_88,_87,_86,_85,_84,_83,_82,_81,_80,_79,_78,_77,_76,_75,_74,_73,_72,_71,_70,_69,_68,_67,_66,_65,_64,_63,_62,_61,_60,_59,_58,_57,_56,_55,_54,_53,_52,_51,_50,_49,_48,_47,_46,_45,_44,_43,_42,_41,_40,_39,_38,_37,_36,_35,_34,_33,_32,_31,_30,_29,_28,_27,_26,_25,_24,_23,_22,_21,_20,_19,_18,_17,_16,_15,_14,_13,_12,_11,_10,_9,_8,_7,_6,_5,_4,_3,_2,_1) (ACTION, __VA_ARGS__)</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> X_PACK_COUNT(LEVEL, ACTION, _99,_98,_97,_96,_95,_94,_93,_92,_91,_90,_89,_88,_87,_86,_85,_84,_83,_82,_81,_80,_79,_78,_77,_76,_75,_74,_73,_72,_71,_70,_69,_68,_67,_66,_65,_64,_63,_62,_61,_60,_59,_58,_57,_56,_55,_54,_53,_52,_51,_50,_49,_48,_47,_46,_45,_44,_43,_42,_41,_40,_39,_38,_37,_36,_35,_34,_33,_32,_31,_30,_29,_28,_27,_26,_25,_24,_23,_22,_21,_20,_19,_18,_17,_16,_15,_14,_13,_12,_11,_10,_9,_8,_7,_6,_5,_4,_3,_2,_1,N,...) LEVEL##N</span></span><br></pre></td></tr></table></figure><p>很离谱第一次看到这种操作，算是奇技淫巧了，分析一下做了什么</p><p>举个例子：<code>x_PACK_N(L,A, myA, myY)</code>扩展成<code>X_PACK_COUNT(L, A, myX, myY, 一坨)(ACTION, __VA_ARGS__)</code></p><p><code>X_PACK_COUNT(L, A, myX, myY, -99, 一坨, _3, _2, _1)</code></p><p><code>X_PACK_COUNT(L, A, _99, _98, 一坨, _2, _1, N)</code></p><p><code>N</code>获取到<code>_2</code></p><p>这个宏相当于是一个截取功能，理解成一个窗口</p><p>这一步的结果是<code>X_PACK_L1_2(X_PACK_L1_DECODE, __VA_ARGS__)</code></p><p>注意：此方法适用于gcc，msvc中需要再添加一个宏<code>#define EXPAND(...) __VA_ARGS__</code>，并且包含在<code>X_PACK_COUNT()</code>之外</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">define</span> EXPEND(X_PACK_COUNT()) EXPEND(ACTION, __VA_ARGS__)</span></span><br></pre></td></tr></table></figure><hr><p>后面经过几步简单的变换转换成</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">X_PACK_L1_DECODE</span>(<span class="built_in">A</span>(uid, <span class="string">&quot;id&quot;</span>)) <span class="built_in">X_PACK_L1_DECODE</span>(<span class="built_in">O</span>(name))</span><br></pre></td></tr></table></figure><p>再来看一下<code>X_PACK_L1_DECODE</code></p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/////////////////////////// XPACK /////////////////////////////</span></span><br><span class="line"><span class="comment">//=======DECODE</span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> X_PACK_L1_DECODE(x)             &#123; X_PACK_L1_DECODE_##x &#125;</span></span><br><span class="line"><span class="comment">//----</span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> X_PACK_L1_DECODE_X(FLAG, ...)   X_EXPAND_FLAG_##FLAG xpack::Extend __x_pack_ext(__x_pack_flag, NULL); X_PACK_N2(X_PACK_L2, X_PACK_DECODE_ACT_O, __VA_ARGS__)</span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> X_PACK_L1_DECODE_E(FLAG, ...)   X_EXPAND_FLAG_##FLAG xpack::Extend __x_pack_ext(__x_pack_flag, NULL); X_PACK_N2(X_PACK_L2, X_PACK_DECODE_ACT_E, __VA_ARGS__)</span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> X_PACK_L1_DECODE_B(FLAG, ...)   X_EXPAND_FLAG_##FLAG xpack::Extend __x_pack_ext(__x_pack_flag, NULL); X_PACK_N2(X_PACK_L2, X_PACK_DECODE_ACT_B, __VA_ARGS__)</span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> X_PACK_L1_DECODE_AF(FLAG, ...)  X_EXPAND_FLAG_##FLAG X_PACK_N2(X_PACK_L2_2, X_PACK_DECODE_ACT_A, __VA_ARGS__) <span class="comment">// extend define in ACTION</span></span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> X_PACK_L1_DECODE_O(...)         X_PACK_L1_DECODE_X(F(0), __VA_ARGS__)</span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> X_PACK_L1_DECODE_M(...)         X_PACK_L1_DECODE_X(F(M), __VA_ARGS__)</span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> X_PACK_L1_DECODE_A(...)         X_PACK_L1_DECODE_AF(F(0), __VA_ARGS__)</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> X_PACK_L1_DECODE_I(...)         X_PACK_N2(X_PACK_L2, X_PACK_DECODE_ACT_I, __VA_ARGS__)</span></span><br><span class="line"><span class="comment">//=======ENCODE</span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> X_PACK_L1_ENCODE(x) &#123; X_PACK_L1_ENCODE_##x &#125;</span></span><br><span class="line"><span class="comment">//-----</span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> X_PACK_L1_ENCODE_X(FLAG, ...)   X_EXPAND_FLAG_##FLAG xpack::Extend __x_pack_ext(__x_pack_flag, NULL); X_PACK_N2(X_PACK_L2, X_PACK_ENCODE_ACT_O, __VA_ARGS__)</span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> X_PACK_L1_ENCODE_E(FLAG, ...)   X_EXPAND_FLAG_##FLAG xpack::Extend __x_pack_ext(__x_pack_flag, NULL); X_PACK_N2(X_PACK_L2, X_PACK_ENCODE_ACT_E, __VA_ARGS__)</span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> X_PACK_L1_ENCODE_B(FLAG, ...)   X_EXPAND_FLAG_##FLAG xpack::Extend __x_pack_ext(__x_pack_flag, NULL); X_PACK_N2(X_PACK_L2, X_PACK_ENCODE_ACT_B, __VA_ARGS__)</span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> X_PACK_L1_ENCODE_AF(FLAG, ...)  X_EXPAND_FLAG_##FLAG X_PACK_N2(X_PACK_L2_2, X_PACK_ENCODE_ACT_A, __VA_ARGS__) <span class="comment">// extend define in ACTION</span></span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> X_PACK_L1_ENCODE_O(...)         X_PACK_L1_ENCODE_X(F(0), __VA_ARGS__)</span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> X_PACK_L1_ENCODE_M(...)         X_PACK_L1_ENCODE_X(F(M), __VA_ARGS__)</span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> X_PACK_L1_ENCODE_A(...)         X_PACK_L1_ENCODE_AF(F(0), __VA_ARGS__)</span></span><br><span class="line"><span class="comment">//-----</span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> X_PACK_L1_ENCODE_I(...)         X_PACK_N2(X_PACK_L2, X_PACK_ENCODE_ACT_I, __VA_ARGS__)</span></span><br></pre></td></tr></table></figure><p>很明显可以看出：用<code>##</code>来实现switch逻辑</p><p><code>X_PACK_L1_DECODE(A(uid, &quot;id&quot;))</code>扩展成<code>&#123;X_PACK_L1_DECODE_A(uid, &quot;id&quot;)&#125;</code></p><hr><p>后面的操作大同小异，经过几次扩展后从转为N2同时L1转为L2再转成<code>X_PACK_ACT</code>类型的宏，最后由如下的宏转换成最终代码</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// ~~~~~~~~~~~~~~~~~~~~~~~~~~~~ decode act ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> X_PACK_DECODE_ACT_O(M)                        \</span></span><br><span class="line"><span class="meta">        __x_pack_ext.vsize = sizeof(__x_pack_self.M); \</span></span><br><span class="line"><span class="meta">        __x_pack_obj.decode(#M, __x_pack_self.M, &amp;__x_pack_ext);</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// enum for not support c++11</span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> X_PACK_DECODE_ACT_E(M)                        \</span></span><br><span class="line"><span class="meta">        __x_pack_ext.vsize = sizeof(__x_pack_self.M); \</span></span><br><span class="line"><span class="meta">        __x_pack_obj.decode(#M, *((int*)&amp;__x_pack_self.M), &amp;__x_pack_ext);</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> X_PACK_DECODE_ACT_A(M, NAME)                                      \</span></span><br><span class="line"><span class="meta">    &#123;                                                                     \</span></span><br><span class="line"><span class="meta">        static xpack::Alias __x_pack_alias(#M, NAME);                     \</span></span><br><span class="line"><span class="meta">        xpack::Extend __x_pack_ext(__x_pack_flag, &amp;__x_pack_alias);       \</span></span><br><span class="line"><span class="meta">        const char *__new_name = __x_pack_alias.Name(__x_pack_obj.Type());\</span></span><br><span class="line"><span class="meta">        __x_pack_ext.vsize = sizeof(__x_pack_self.M);                     \</span></span><br><span class="line"><span class="meta">        __x_pack_obj.decode(__new_name, __x_pack_self.M, &amp;__x_pack_ext);  \</span></span><br><span class="line"><span class="meta">    &#125;</span></span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;C-高级宏操作&quot;&gt;&lt;a href=&quot;#C-高级宏操作&quot; class=&quot;headerlink&quot; title=&quot;C++高级宏操作&quot;&gt;&lt;/a&gt;C++高级宏操作&lt;/h1&gt;&lt;p&gt;最近在看一个c++各数据类型和 JSON 或 xml 互转的库，重点阅读了其中宏的编写&lt;/p&gt;
</summary>
      
    
    
    
    <category term="C++" scheme="http://example.com/categories/C/"/>
    
    
    <category term="C++" scheme="http://example.com/tags/C/"/>
    
  </entry>
  
  <entry>
    <title>Gamma校正详解</title>
    <link href="http://example.com/2021/08/23/Gamma%E6%A0%A1%E6%AD%A3%E8%AF%A6%E8%A7%A3/"/>
    <id>http://example.com/2021/08/23/Gamma%E6%A0%A1%E6%AD%A3%E8%AF%A6%E8%A7%A3/</id>
    <published>2021-08-23T09:16:00.000Z</published>
    <updated>2023-09-13T06:01:36.617Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Gamma校正详解"><a href="#Gamma校正详解" class="headerlink" title="Gamma校正详解"></a>Gamma校正详解</h1><p><a href="https://www.cambridgeincolour.com/tutorials/gamma-correction.htm">参考阅读</a></p><p><a href="https://learnopengl.com/Advanced-Lighting/Gamma-Correction">LearnOpenGL</a></p><p>首先，物理上灰度的变化是平滑的，两倍的光子就产生两倍的灰度变化，是线性的，但是在人眼中就不同了，人眼对暗部更加敏感，是非线性的</p><p>打个比方就是<strong>在暗处 <strong>0.5 倍的物理光子变化，在人眼中可能变化了 2-3 倍，而</strong>在亮处</strong>，要让人感到 2 倍的亮度变化，光子可能需要增加 3-4 倍</p><p><img src="/iimmgg/gamma_correction_brightness.png" alt="gamma_correction_brightness"></p><p>如上图，线性空间（物理空间）中的 0.2 在人看来就是美术意义上的中灰</p><p><img src="/iimmgg/gamma_correction_gamma_curves.png" alt="gamma_correction_gamma_curves"></p><h2 id="简易的一个例子"><a href="#简易的一个例子" class="headerlink" title="简易的一个例子"></a>简易的一个例子</h2><p>假设自然界中一个 0.218 的灰度，在人眼中就是 0.5</p><p>用相机等光学元件记录时，记录的就是 0.218（大多数数码相机以线性方式记录光线）</p><p>经过 gamma 校正（gamma 编码），0.218 通过传递函数以 0.5 存储在硬盘中（这一步在现实中，由绘图软件本身做了，存储在 jpg 等格式的文件中）</p><p>显示时经 gamma2.2 压暗，以 0.218 的光显示，人眼中就是 0.5，和最开始直接观察一致</p><hr><ul><li>如果<strong>不校正</strong>直接存储在硬盘中，经过 gamma2.2 的压暗输出 0.035，人眼中就是 0.218，结果我们<strong>直接看到了线性空间</strong>，就是图一的第二行，导致图片的视觉效果非常亮，而且在人眼中的黑到灰被压缩到了 0.0-0.2，图片的暗部没有细节，而且从黑到白的过度非常生硬有跳跃感，下面是在 unity 中做的效果</li></ul><p><img src="/iimmgg/QQ图片20210822140930.png" alt="QQ图片20210822140930" style="zoom: 67%;" /><img src="/iimmgg/QQ图片20210822140945.png" alt="QQ图片20210822140945" style="zoom: 67%;" />  </p><p>左图Gamma校正，右图无Gamma校正，灰色过度几乎没了，由于只是简单的光照模型没用贴图，所以不能直观感受暗部的细节缺失</p><hr><h2 id="sRGB-贴图过曝问题"><a href="#sRGB-贴图过曝问题" class="headerlink" title="sRGB 贴图过曝问题"></a>sRGB 贴图过曝问题</h2><p>sRGB空间 在内存中就是 <code>1/2.2</code>的伽马编码，非线性的编码</p><p>当我们在渲染器中使用gamma校正后，就是对原图又进行了一次亮度提升，特别明显的是超出 1.0 的部分被截断在 1 导致大量的纯白</p><p>另一个就是混合操作时，直接拿非线性编码混合就出问题了，需要<strong>转换到线性空间下再参与计算</strong></p><figure class="highlight glsl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">float</span> gamma = <span class="number">2.2</span>;</span><br><span class="line"><span class="type">vec3</span> linearColor = <span class="built_in">pow</span>(<span class="built_in">texture</span>(texture0, Texcoords).rgb, <span class="type">vec3</span>(gamma));</span><br></pre></td></tr></table></figure><hr><p>并非所有纹理实际上都在 sRGB 空间中。用于为对象着色的纹理（如漫反射纹理）几乎总是在 sRGB 空间中。用于检索光照参数的纹理（如镜面反射贴图和法线贴图）几乎总是在线性空间中</p><hr><h2 id="光照衰减"><a href="#光照衰减" class="headerlink" title="光照衰减"></a>光照衰减</h2><p>由于 gamma 校正的影响</p><p>一句话总结：线性空间用线性，伽马空间用物理方程</p><p>线性空间中用经验方程<br>$$<br>线性空间中的变化\frac{1}{D}<br>$$</p><p>$$<br>经过显示gamma后，显示亮度为\left(\frac{1}{D}\right)^{2.2}&#x3D;\frac{1}{D^{2.2}}，差不多2次幂<br>$$</p><hr><h2 id="unity对sRGB图像的处理"><a href="#unity对sRGB图像的处理" class="headerlink" title="unity对sRGB图像的处理"></a>unity对sRGB图像的处理</h2><p>贴图选项勾选 sRGB 在采样时就将伽马空间中的数值转到线性空间中了（重校），经过着色器操作，最后输出到屏幕之前通过 sRGB Frame Buffer 进行 gamma 校正以平衡显示器的显示gamma</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;Gamma校正详解&quot;&gt;&lt;a href=&quot;#Gamma校正详解&quot; class=&quot;headerlink&quot; title=&quot;Gamma校正详解&quot;&gt;&lt;/a&gt;Gamma校正详解&lt;/h1&gt;&lt;p&gt;&lt;a href=&quot;https://www.cambridgeincolour.com</summary>
      
    
    
    
    <category term="图形学" scheme="http://example.com/categories/%E5%9B%BE%E5%BD%A2%E5%AD%A6/"/>
    
    
    <category term="Graphics" scheme="http://example.com/tags/Graphics/"/>
    
  </entry>
  
</feed>
